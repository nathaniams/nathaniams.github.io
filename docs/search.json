[
  {
    "objectID": "FinalProject.html#group-1",
    "href": "FinalProject.html#group-1",
    "title": "Final Project",
    "section": "Group 1:",
    "text": "Group 1:\n\nHiba Awan\nNathania Stephens"
  },
  {
    "objectID": "FinalProject.html#motivation-purpose",
    "href": "FinalProject.html#motivation-purpose",
    "title": "Final Project",
    "section": "Motivation/ Purpose",
    "text": "Motivation/ Purpose\nIn 2023, there were over 30,000 arrests and close to 65,000 citations in Fairfax County. The Fairfax County boundaries, include areas such as Centreville, Chantilly, Herndon, Reston, Tysons Corner, McLean, Merrifield, George Mason, Annadale, Burke, Springfield, Alexandria, Lorton to name a few. If you live, work, or study in these areas then this project should be of interest to you. This project aims to inform Fairfax County patrons of crime information and hopefully provide some statistical insights that could be applicable."
  },
  {
    "objectID": "FinalProject.html#goals-objectives",
    "href": "FinalProject.html#goals-objectives",
    "title": "Final Project",
    "section": "Goals/ Objectives",
    "text": "Goals/ Objectives\nIn order to provide relevant and insightful crime information, several different visualization methods were applied to help easily interpret and compare data. Statistical learning techniques were utilized to help understand statistic significantly factors and associations between variables. Since the data utilized for this project is largely categorical the project focuses on techniques such as Chi-Squared Test, Logistic Regression, Decision Trees and Random Forest."
  },
  {
    "objectID": "FinalProject.html#overview",
    "href": "FinalProject.html#overview",
    "title": "Final Project",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "FinalProject.html#about-the-data",
    "href": "FinalProject.html#about-the-data",
    "title": "Final Project",
    "section": "About the Data",
    "text": "About the Data\nThree datasets were pulled from the Fairfax County Police Department website. They covered arrest, citations, and warnings in the year 2023. For simplicity, general definitions are provided:\n\nArrest - When a person is taken into custody to answer for an offense or when there is a deprivation or restraint of a person’s liberty in any significant way.\nCitation - Formal notice issued by law enforcement officer for a violation of law, typically related to traffic laws or other minor offenses. Typically requiring a violator to appear in court or pay a fine.\nWarning - When a violation, typically minor, has been made but an officer issues a warning rather than a citation.\n\nThe data sets included between 24 and 34 variables, but some of many of the variables were redundant or were not applicable to the research (e.g. web_address, phone_number, name). The following attributes were key to the research conducted:\n\n\n\n\n\n\n\n\nColumn Name\nData Type\nDescription\n\n\n\n\nDate\nDate\nDate of Violation\n\n\nTime\nChr\nTime of Violation\n\n\nOffense\nChr\nDescription of Violation\n\n\nGender\nChr\nGender of Violator\n\n\nEthnicity\nChr\nHispanic or Non-Hispanic\n\n\nDistrict\nChr\nAdministrative area\n\n\nLatitude\nDbl\nCoordinates measuring north/ south of equator\n\n\nLongitude\nDbl\nCoordinates measuring east/ west of prime meridian\n\n\nOutcome\nChr\nResult of violation, arrest, citation, or warning"
  },
  {
    "objectID": "FinalProject.html#limitations-and-assumptions",
    "href": "FinalProject.html#limitations-and-assumptions",
    "title": "Final Project",
    "section": "Limitations and Assumptions",
    "text": "Limitations and Assumptions\nDue to the nature of the data available on the Fairfax County Police Department website, analysis was limited to qualitative techniques. The approach taken for the project focused on predicting through qualitative responses or classification. This means that each record pulled from the Fairfax County Police Department (FCPD) would be assigned to a category or class.\nWhile understanding local crime is the goal of this project, the data acquired only accounts for crime that was recorded by FCPD. It does not take into account crimes that were not report or any other crime that was not reported through FCPD channels."
  },
  {
    "objectID": "FinalProject.html#cleaning-and-transformation",
    "href": "FinalProject.html#cleaning-and-transformation",
    "title": "Final Project",
    "section": "Cleaning and Transformation",
    "text": "Cleaning and Transformation\nTo address questions related to gender, the data needed to be standardized and correctly categorized. Column names needed to be consistent across the three datasets to merge. Gender was used over Sex. Next the column data would be transformed to consistent labels, e.g. Male, Female, and Other/Unknown. Total proportion for Gender was examined, to verify that other/ unknown class could be removed without…"
  },
  {
    "objectID": "FinalProject.html#research-analysis",
    "href": "FinalProject.html#research-analysis",
    "title": "Final Project",
    "section": "Research & Analysis",
    "text": "Research & Analysis\n\nQuestion 1: Is there an association between gender and warnings?\nTo address this question the null and alternative hypothesis are established.\nNull Hypothesis: There is no association between gender and violation outcome, warning or citation. This would mean that the likelihood of a violator getting a warning is independent of gender.\nAlternative Hypothesis: There is an association between gender and violation outcome. This implies that gender affects the outcome of whether a violator is given a citation or warning.\nAccording to the cleaned and combined dataset for warnings and citation, there was a total of 88,320 records. By looking at the counts for each outcome (citation or warning), there are a lot more citations than there are warnings given out by FCPD. This stacked bar chart also shows that males have a higher count for both categories.\n\n\n\n\n\n\n\n\n\nNext, the warning rate for gender is calculated. This looks at the probability of a male or female violator receiving a Warning instead of a citation e.g. getting out of a ticket. To calculate warning rate, the number of warnings are divided by the total number of incidents.\n\\[\n\\begin{align*}\nWarning Rate = \\frac{\\text{Number of Warnings}}{\\text{Total Incidents (Warnings + Citations)}}\n\\end{align*}\n\\]\nThis shows a slight difference in proportion between the two genders, with females having a higher warning rate than males. In other words, females received more warnings than males. Is this difference significant or is it a result of chance or other factors? To help understand these results, the Chi-Square Test of Independence is used. The Chi-Square Test of Independence will help determine whether the variables, gender and outcome, are independent or if there is a relationship between them.\n\n\n\n\n\n\n\n\n\n\\[\n\\chi^2 = \\sum \\frac{(O-E)^2}{E}\n\\] To implement the Chi-Square Test, a contingency table is generated, which shows the distribution for gender and outcome.\n\n\n\nGender\nCitations\nWarnings\n\n\n\n\nFemale\n20,478\n8,777\n\n\nMale\n43,657\n15,408\n\n\n\nThe results of the Chi-Square test shows:\n\nChi-Square Statistic (x-squared): The chi-square test statistic is 150.62. This is the discrepancy between the observed frequencies, citations and warnings, and the expected frequencies if there were no association between the gender and outcome. This is demonstrated in the below tables.\n\n\n\n\n\n\n\n\n\n\n\nGender\nExpected Citations\nObserved Citations\nExpected Warnings\nObserved Warnings\n\n\n\n\nFemale\n21,243\n20,478\n8,011\n8,777\n\n\nMale\n42,891\n43,657\n16,173\n15,408\n\n\n\n\nDegrees of Freedom (df): The degree of freedom for this test is 1, which is the number of rows minus 1 multiplied by number of columns minus 1.\np-value: The p-value is 2.2e-16 which is much smaller than 0.05. This represents the probability of observing the chi-square statistic, 150.62, or more if the null hypothesis were true.\n\nTo visualize each value in the above table by its contribution to the chi-square test a heatmap is generated. This quickly shows which values have the highest contribution percentage.\n\n\n\n\n\n\n\n\n\nThus the null hypothesis is rejected. The results show that there is a statistically significant association between gender and violation outcome in Fairfax County. In other words, the Chi-square test indicates that the likelihood of a violation outcome is significantly associated with gender.\n\n\nQuestion #: Does Time of Day or Day of the Month Factor into Number of Citations?\nTo examine the likelihood of a getting a citation versus a warning (outcome of a violation), a heatmap is generated to understand how this behavior looks during each day of the month and hour of the day (24-hour clock). To calculate the citation rate, the following equation is used.\n\\[\n\\begin{align*}\nCitation Rate = \\frac{\\text{Number of Citations}}{\\text{Total Incidents (Warnings + Citations)}}\n\\end{align*}\n\\]\nThe higher intensity or darker color areas represent a higher citation rate. The citation rate represents the number of citations divided by the total outcome which includes both warnings and citations. During the hours of 0500-0600 there appears to be less warnings issued and instead citations issues. This also corresponds to morning rush times.\n\n\n\n\n\n\nUsing Logistic Regression… will update this with cleaner numbers….\n\n\n# A tibble: 88,320 × 7\n   BinaryOutcome Hour  DayOfMonth DayOfWeek District    Gender Race \n   &lt;fct&gt;         &lt;fct&gt; &lt;fct&gt;      &lt;ord&gt;     &lt;fct&gt;       &lt;fct&gt;  &lt;fct&gt;\n 1 0             16    12         Wed       Dranesville Male   W    \n 2 0             15    13         Mon       Providence  Male   B    \n 3 0             15    13         Mon       Providence  Male   B    \n 4 0             15    13         Mon       Providence  Male   B    \n 5 0             5     9          Thu       Sully       Female B    \n 6 0             0     10         Mon       Franconia   Male   W    \n 7 0             1     10         Mon       Franconia   Male   A    \n 8 0             13    11         Tue       Dranesville Male   B    \n 9 0             11    31         Tue       Hunter Mill Male   B    \n10 0             11    10         Mon       Hunter Mill Female W    \n# ℹ 88,310 more rows\n\n\n\nCall:  glm(formula = model_formula_2, family = binomial(link = \"logit\"), \n    data = data_for_modeling)\n\nCoefficients:\n         (Intercept)                 Hour1                 Hour2  \n           -0.543953              0.067872              0.138748  \n               Hour3                 Hour4                 Hour5  \n            0.322492             -0.134056             -0.715503  \n               Hour6                 Hour7                 Hour8  \n           -0.928240             -0.183564             -0.186012  \n               Hour9                Hour10                Hour11  \n           -0.216679             -0.281275             -0.374341  \n              Hour12                Hour13                Hour14  \n           -0.299027             -0.369616             -0.369130  \n              Hour15                Hour16                Hour17  \n           -0.257313             -0.357045             -0.343120  \n              Hour18                Hour19                Hour20  \n           -0.209459              0.024723              0.208840  \n              Hour21                Hour22                Hour23  \n            0.263136              0.211006              0.046055  \n         DayOfMonth2           DayOfMonth3           DayOfMonth4  \n           -0.017060             -0.284725             -0.226471  \n         DayOfMonth5           DayOfMonth6           DayOfMonth7  \n           -0.307410              0.095658              0.016076  \n         DayOfMonth8           DayOfMonth9          DayOfMonth10  \n           -0.155587             -0.280091             -0.159569  \n        DayOfMonth11          DayOfMonth12          DayOfMonth13  \n           -0.285784             -0.294318             -0.116350  \n        DayOfMonth14          DayOfMonth15          DayOfMonth16  \n           -0.110178              0.116435              0.008238  \n        DayOfMonth17          DayOfMonth18          DayOfMonth19  \n           -0.196301             -0.244815             -0.065312  \n        DayOfMonth20          DayOfMonth21          DayOfMonth22  \n           -0.188286             -0.158645             -0.150123  \n        DayOfMonth23          DayOfMonth24          DayOfMonth25  \n           -0.172025             -0.015141             -0.373595  \n        DayOfMonth26          DayOfMonth27          DayOfMonth28  \n           -0.299717             -0.205006             -0.131540  \n        DayOfMonth29          DayOfMonth30          DayOfMonth31  \n           -0.315680             -0.224334              0.006383  \n         DayOfWeek.L           DayOfWeek.Q           DayOfWeek.C  \n            0.475198              0.108194              0.056998  \n         DayOfWeek^4           DayOfWeek^5           DayOfWeek^6  \n           -0.125991             -0.038477             -0.074402  \n DistrictDranesville     DistrictFranconia   DistrictHunter Mill  \n            0.182899              0.416758              0.135421  \n       DistrictMason  DistrictMount Vernon    DistrictProvidence  \n            0.333657              0.591765              0.168673  \n DistrictSpringfield         DistrictSully            GenderMale  \n           -0.249097             -0.196909             -0.206635  \n               RaceB                 RaceI                 RaceU  \n            0.141369              0.003108             -0.325427  \n               RaceW  \n           -0.042346  \n\nDegrees of Freedom: 88319 Total (i.e. Null);  88247 Residual\nNull Deviance:      103700 \nResidual Deviance: 99940    AIC: 100100\n\n\n                               Variable OddsRatio\nHour6                             Hour6 0.3952486\nHour5                             Hour5 0.4889459\nHour11                           Hour11 0.6877422\nDayOfMonth25               DayOfMonth25 0.6882558\nHour13                           Hour13 0.6909997\nHour14                           Hour14 0.6913355\nHour16                           Hour16 0.6997413\nHour17                           Hour17 0.7095532\nRaceU                             RaceU 0.7222187\nDayOfMonth29               DayOfMonth29 0.7292931\nDayOfMonth5                 DayOfMonth5 0.7353494\nDayOfMonth26               DayOfMonth26 0.7410280\nHour12                           Hour12 0.7415395\nDayOfMonth12               DayOfMonth12 0.7450394\nDayOfMonth11               DayOfMonth11 0.7514248\nDayOfMonth3                 DayOfMonth3 0.7522211\nHour10                           Hour10 0.7548205\nDayOfMonth9                 DayOfMonth9 0.7557153\nHour15                           Hour15 0.7731266\nDistrictSpringfield DistrictSpringfield 0.7795040\nDayOfMonth18               DayOfMonth18 0.7828495\nDayOfMonth4                 DayOfMonth4 0.7973425\nDayOfMonth30               DayOfMonth30 0.7990478\nHour9                             Hour9 0.8051887\nHour18                           Hour18 0.8110225\nGenderMale                   GenderMale 0.8133167\nDayOfMonth27               DayOfMonth27 0.8146427\nDistrictSully             DistrictSully 0.8212650\nDayOfMonth17               DayOfMonth17 0.8217645\nDayOfMonth20               DayOfMonth20 0.8283775\n\n\n                                 Variable OddsRatio\nDistrictMount Vernon DistrictMount Vernon 1.8071754\nDayOfWeek.L                   DayOfWeek.L 1.6083326\nDistrictFranconia       DistrictFranconia 1.5170354\nDistrictMason               DistrictMason 1.3960644\nHour3                               Hour3 1.3805635\nHour21                             Hour21 1.3010033\nHour22                             Hour22 1.2349197\nHour20                             Hour20 1.2322479\nDistrictDranesville   DistrictDranesville 1.2006927\nDistrictProvidence     DistrictProvidence 1.1837330\nRaceB                               RaceB 1.1518494\nHour2                               Hour2 1.1488349\nDistrictHunter Mill   DistrictHunter Mill 1.1450188\nDayOfMonth15                 DayOfMonth15 1.1234850\nDayOfWeek.Q                   DayOfWeek.Q 1.1142641\nDayOfMonth6                   DayOfMonth6 1.1003830\nHour1                               Hour1 1.0702283\nDayOfWeek.C                   DayOfWeek.C 1.0586540\nHour23                             Hour23 1.0471321\nHour19                             Hour19 1.0250314\nDayOfMonth7                   DayOfMonth7 1.0162059\nDayOfMonth16                 DayOfMonth16 1.0082723\nDayOfMonth31                 DayOfMonth31 1.0064030\nRaceI                               RaceI 1.0031125\nDayOfMonth24                 DayOfMonth24 0.9849729\nDayOfMonth2                   DayOfMonth2 0.9830843\nDayOfWeek^5                   DayOfWeek^5 0.9622539\nRaceW                               RaceW 0.9585381\nDayOfMonth19                 DayOfMonth19 0.9367755\nDayOfWeek^6                   DayOfWeek^6 0.9282980\n\n\n\n\nAnother Questions….\nTo address each of these question, first exploratory analysis should be done to gain an understanding and summary of the crime metrics for Fairfax County. This includes understanding what type of crimes occurred the most and where.\nGeneral crime Mapping the arrest data for a geospatial visual of where arrest occur.\n\n\n\n\n\n\nNext we look at the Top 10 Arrest Type by Incident Based Reporting (IBR) codes."
  },
  {
    "objectID": "Resources.html",
    "href": "Resources.html",
    "title": "Helpful Links",
    "section": "",
    "text": "Quarto websites https://quarto.org/docs/websites."
  },
  {
    "objectID": "Resources.html#helpful-sites",
    "href": "Resources.html#helpful-sites",
    "title": "Helpful Links",
    "section": "",
    "text": "Quarto websites https://quarto.org/docs/websites."
  },
  {
    "objectID": "Redesign.html",
    "href": "Redesign.html",
    "title": "Redesign Project",
    "section": "",
    "text": "For this project, an Outforia 2022 article on “The USA’s Search and Rescue Hotspots,” was chosen. Within the article there are four tables consisting of the highest numbers of Search and Rescue (SAR) incidents grouped by either National Parks or States. [1] For this project, two of the four tables were chosen for a redesign.\nThe article focuses on Search and Rescue incidents that occurred between 2018-2020 within US National Parks. Considering the context and the author’s statement at the bottom of the article, this article and tables were likely created for readers who enjoy the outdoors, hiking, exploring and soaking in the beauty of national parks. It is also likely that the article is intended to help novice outdoor adventurers by providing awareness into the highest number of search and rescue events.This helps readers make informed decisions regarding their next outdoor adventure!\nWhile the article is informative and helps readers understand which US National Parks could be considered most dangerous, the goal of this project was to enhance the visualization through best practices and improved graph design using R.\nThe first table provides the top 16 national parks with the highest number of incidents between 2018-2020. Some of the disadvantages of this table is that it is a table, while simple and easy to read, it does not provide quick comparison like a bar chart would offer. Another consideration is the unnecessary visual at the top of the table. Above the table are the three parks with the highest number of incidents. The only additional information provided are the states the parks reside in; however, this is easily overlooked based on their position, color, and pretty yet distracting pictures.\n\n\n\n\n\n\n\nNPS Most Search and Rescues by Outforia\n\n\nThe second table provides the top 20 US States with the most search and rescue incidents. Yes, Virginia made the list. In this table we have more information on total search and rescue incidents, but grouped by State instead of individual parks. This provides a holistic view of incidents across a given state. Again we are given a table, which doesn’t offer fast comparison and the top three states which offer no additional information from the table but does offer pictures of the states’ flags and an outline of the state.\n\n\n\n\n\n\n\nStates Most Search and Rescue by Outforia"
  },
  {
    "objectID": "Redesign.html#the-original-design",
    "href": "Redesign.html#the-original-design",
    "title": "Redesign Project",
    "section": "",
    "text": "For this project, an Outforia 2022 article on “The USA’s Search and Rescue Hotspots,” was chosen. Within the article there are four tables consisting of the highest numbers of Search and Rescue (SAR) incidents grouped by either National Parks or States. [1] For this project, two of the four tables were chosen for a redesign.\nThe article focuses on Search and Rescue incidents that occurred between 2018-2020 within US National Parks. Considering the context and the author’s statement at the bottom of the article, this article and tables were likely created for readers who enjoy the outdoors, hiking, exploring and soaking in the beauty of national parks. It is also likely that the article is intended to help novice outdoor adventurers by providing awareness into the highest number of search and rescue events.This helps readers make informed decisions regarding their next outdoor adventure!\nWhile the article is informative and helps readers understand which US National Parks could be considered most dangerous, the goal of this project was to enhance the visualization through best practices and improved graph design using R.\nThe first table provides the top 16 national parks with the highest number of incidents between 2018-2020. Some of the disadvantages of this table is that it is a table, while simple and easy to read, it does not provide quick comparison like a bar chart would offer. Another consideration is the unnecessary visual at the top of the table. Above the table are the three parks with the highest number of incidents. The only additional information provided are the states the parks reside in; however, this is easily overlooked based on their position, color, and pretty yet distracting pictures.\n\n\n\n\n\n\n\nNPS Most Search and Rescues by Outforia\n\n\nThe second table provides the top 20 US States with the most search and rescue incidents. Yes, Virginia made the list. In this table we have more information on total search and rescue incidents, but grouped by State instead of individual parks. This provides a holistic view of incidents across a given state. Again we are given a table, which doesn’t offer fast comparison and the top three states which offer no additional information from the table but does offer pictures of the states’ flags and an outline of the state.\n\n\n\n\n\n\n\nStates Most Search and Rescue by Outforia"
  },
  {
    "objectID": "Redesign.html#about-the-data",
    "href": "Redesign.html#about-the-data",
    "title": "Redesign Project",
    "section": "About the data",
    "text": "About the data\nTo obtain the data Outforia submitted a freedom of information request to the National Park Authority for search and rescue incidents between 2018-2020 for all US national parks. In cases where national parks spanned more than one state, Outforia counted the number of incidents for each state. [1] In this redesign project, data from the tables were used.\nInformation from National Park Services (NPS) site was used to supplement additional information. [2] For example, adding state to the national parks data and adding coordinates to allow for plotting on the interactive map."
  },
  {
    "objectID": "Redesign.html#purpose-of-redesign",
    "href": "Redesign.html#purpose-of-redesign",
    "title": "Redesign Project",
    "section": "Purpose of Redesign",
    "text": "Purpose of Redesign\nThe objectives of redesigning these tables are to provide informative graphs to help readers further interpret the data. By ensuring the graphs focus on accurate comparisons, simplified and uncluttered graphs, and appropriate context to keep the readers engaged. This will help readers make informed decisions about their next visit to a National Park.\n\nRedesign of Table 1\nIn this redesign, a horizontal bar chart is used. This allows for easy reading of the park names while still arranging the number of incidents from high to low. Also, this chart provides quick comparison for readers to easily compare bar lengths. In the book, Storytelling with data, “Our eyes compare the end points of the bars, so it is easy to see quickly which category is the biggest, which is the smallest, and also the incremental difference between categories.” [3]\n\n\n\n\n\n\n\n\n\n\n\nRedesign of Table 1, another look\nIn this redesign, table 1 is transformed to provide a map view of the data. This takes a different approach to understanding the search and rescue incidents from table 1. Here we are still using the same data from table 1, but instead of representing the park the state is used instead to represent states with the most incidents. This is another way to visualize the information from table 1. Another consideration is that some National Parks span more than one state. In this map, only one state was selected for each National Park. If data was available for each of the search and rescue events, then we could plot those events and have a more exact number of what state the incidents occurred in.\n\n\n\n\n\n\n\n\n\n\n\nRedesign of Table 1, a better look\nHere is another way to visualize the information that blends location while still providing the number of Search and Rescue Incidents for the top parks. The use of an interactive map is to allow readers not only to understand the parks with the highest number of incidents, but to help explore where those parks are and how large they area. The interactive map allows the reader to zoom in and out and pan around the map. Each marker has the name of the Park and the number of incidents. *Note: Sequoia National Park and Kings Canyon National Park were originally combined in the table. However, the NPS has them as separate parks.Therefore the number of incidents were divide between the two parks.\n\n\n\n\n\n\n\n\nRedesign of Table 2\nIn table 2 we see 20 states listed with the total number of Search and Rescue incidents from 2018-2020. Here it seems like the visual is on the right track; however, after counting the states there are only 19 highlighted! Oh yes, sneaky Alaska and Hawaii are not represented in this map.\n\n\n\n\n\n\n\n\n\nIn order to get the two states in the chart, without zooming too far out and losing the state-level resolution, another approach to the map view must be taken using library(usmap) and library(sf). Now we have all top 20 states properly represented on the map. To reduce clutter and unnecessary information [3], the x-axis and y-axis were removed. This is not only good practice to reduce clutter but another consideration is the placement of Alaska, Hawaii, and Puerto Rico. They are placed toward the bottom of the map view to include them in the graph not to represent their actual locations.\n\n\n\n\n\n\n\n\n\nInitially, an unusual error occurred when trying to render from R Studio to html. The map was missing the data overlay. The map generated without issues in R Studio. For a workaround, I saved the ggplot as an image and included in the qmd. After revisiting this issue on the Redesign Code page, the issue was discovered in the variable name for the dataframe that was being used. After renaming the variable the issue was resolved."
  },
  {
    "objectID": "Redesign.html#video-presentation",
    "href": "Redesign.html#video-presentation",
    "title": "Redesign Project",
    "section": "Video Presentation",
    "text": "Video Presentation\nThis is a test."
  },
  {
    "objectID": "Redesign.html#references",
    "href": "Redesign.html#references",
    "title": "Redesign Project",
    "section": "References",
    "text": "References\n\n[1]\nBorg, C., 2022, “The USA’s Search And Rescue Hotspots,” Outforia. [Online]. Available: https://outforia.com/search-and-rescue-hotspots/\n\n\n[2]\n2024, “NPS – Land Resources Division Boundary and Tract Data Service,” National Park Service. [Online]. Available: https://public-nps.opendata.arcgis.com/maps/c8d60ffcbf5c4030a17762fe10e81c6a/about\n\n\n[3]\nKnaflic, C. N., 2015. Storytelling with data: a data visualization guide for business professionals (1st ed.). Wiley."
  },
  {
    "objectID": "FinalProjectCode.html",
    "href": "FinalProjectCode.html",
    "title": "Final Project Code",
    "section": "",
    "text": "Final Project Code…\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(leaflet)\nlibrary(usmap)\nlibrary(sf)\nlibrary(readr)\nlibrary(scales)\nlibrary(tidyr)\n\nArrest2023 &lt;- read_csv(\"2023_arrest_data.csv\", \n                       col_types = cols(ArresteeNumber = col_skip(), \n                                        ArrestDate = col_date(format = \"%m/%d/%Y\"), \n                                        ArrestTime = col_character(), WEB_ADDRESS = col_skip(), \n                                        PHONE_NUMBER = col_skip(), NAME = col_skip()))\n\n## Remove any outliers by setting min/max lat and long.\nmin_lat = 38.6\nmax_lat = 39.0\nmin_lon = -77.6\nmax_lon = -77.0\n\n## Using all data but excluding any coordinates that are not within coordinates\narrest_data_clean = Arrest2023 %&gt;%\n  filter(\n    Latitude &gt;= min_lat,\n    Latitude &lt;= max_lat,\n    Longitude &gt;= min_lon,\n    Longitude &lt;= max_lon\n  )\n\n## Include district boundaries by reading in shape file downloaded from Fairfax County site\ndistrict_boundaries = st_read(\"Supervisor_Districts/Supervisor_Districts.shp\")\n\nReading layer `Supervisor_Districts' from data source \n  `C:\\Users\\Nat\\OneDrive - George Mason University - O365 Production\\Git\\nathaniams.github.io\\Supervisor_Districts\\Supervisor_Districts.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 9 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 11757190 ymin: 6905421 xmax: 11899000 ymax: 7070364\nProjected CRS: NAD83 / Virginia North (ftUS)\n\n## Transform to WGS84 projection to match basemap within leaflet\nif (st_crs(district_boundaries)$epsg != 4326) {\n  district_boundaries = st_transform(district_boundaries, 4326)\n}\n\n\n## Using leaflet to map the Arrest data and include district layers\nleaflet() %&gt;%\n  addProviderTiles(providers$OpenStreetMap) %&gt;%\n  addPolygons(\n    data = district_boundaries,\n    fillOpacity = 0.5,\n    color = \"black\",\n    weight = 1,\n    popup = ~paste(\"District:\", DISTRICT)\n  ) %&gt;%\n  addCircleMarkers(\n    data = arrest_data_clean,\n    lng = ~Longitude,\n    lat = ~Latitude,\n    radius = 3,\n    color = \"darkred\",\n    stroke = FALSE,\n    fillOpacity = 0.4,\n    popup = ~paste(\n      \"Case Number: \", CaseNumber, \"&lt;br&gt;\",\n      \"Arrest Type: \", IBRDescription)\n  )\n\n\n\n\n\n\n# By IBRCode\narrest_count = Arrest2023 %&gt;%\n  group_by(IBRCode) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(Count))\n\n# Get top 10\ntop_10_arrest_IBR = head(arrest_count, 10)\n\n# Add codes - General Description\nIBRCodeDetails = read.csv(\"IBRcodes.csv\")\n\n# Reassign the codes\ntop_10_arrest_decoded = top_10_arrest_IBR %&gt;%\n  left_join(IBRCodeDetails, by = c(\"IBRCode\" = \"CODE\")) %&gt;%\n  select(IBRCode, OFFENSE, everything())\n\ntop_10_arrest_cleaned = top_10_arrest_decoded %&gt;%\n  distinct(IBRCode, OFFENSE, .keep_all = TRUE)\n\n# Decode for cleaner results\narrest_chart = ggplot(top_10_arrest_cleaned, aes(x = reorder(OFFENSE, Count),\n                                              y = Count)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 Arrest Type by IBR decoded\",\n       x = \"Offense by IBR Code\",\n       y = \"Number of Arrests\") +\n  theme_grey() +\n  geom_text(aes(label = Count), hjust = -0.1, size = 3.5) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15)))\n\narrest_chart\n\n\n\n\n\n\n\n\n\nlibrary(readr)\nlibrary(lubridate)\n\nwarnings = read_csv(\"2023_warning_data.csv\", \n                         col_types = cols(Warnings_Date = col_date(format = \"%m/%d/%Y\"), \n                                          WEB_ADDRESS = col_skip(), PHONE_NUMBER = col_skip(), \n                                          NAME = col_skip()))\n\ncitations = read_csv(\"2023_citation_data.csv\", \n                                col_types = cols(Date = col_date(format = \"%m/%d/%Y\"), \n                                                 WEB_ADDRESS = col_skip(), PHONE_NUMBER = col_skip(), \n                                                 NAME = col_skip()))\n\n# Rename some columns\ncitations = citations %&gt;%\n  rename(ViolationDate = Date)\n\n# change Gender to sex in warnings and change date column name\nwarnings = warnings %&gt;%\n  rename(Sex = Gender)\n\nwarnings = warnings %&gt;%\n  rename(ViolationDate = Warnings_Date)\n\n# Adjust Citations and prepare for Merge\n# Assumption that ID is the officer's ID\ncitations_processed = citations %&gt;%\n  mutate(\n    outcome = \"Citation\",\n    Gender = case_when(\n      Sex == \"M\" ~ \"Male\",\n      Sex == \"F\" ~ \"Female\",\n      TRUE ~ \"Other/Unknown\"\n    ),\n    Year = year(ViolationDate),\n    Month = month(ViolationDate),\n    DayOfMonth = day(ViolationDate),\n    Time = parse_date_time(Time, \"HM\"),\n    data_type = \"Citation\"\n  ) %&gt;%\n  select(\n    outcome, Gender, Year, Month, DayOfMonth, Time, Offense_Description = Charge,\n    District = DISTRICT, Race, Ethnicity, Latitude, Longitude, OfficerID = ID, data_type\n  )\n\n# Adjust Warnings and prepare for Merge\nwarnings_processed = warnings %&gt;%\n  mutate(\n    outcome = \"Warning\",\n    Gender = case_when(\n      Sex == \"M\" ~ \"Male\",\n      Sex == \"F\" ~ \"Female\",\n      TRUE ~ \"Other/Unknown\"\n    ),\n    Year = year(ViolationDate),\n    Month = month(ViolationDate),\n    DayOfMonth = day(ViolationDate),\n    Time = parse_date_time(Time, \"HM\"),\n    data_type = \"Warning\"\n  ) %&gt;%\n  select(\n    outcome, Gender, Year, Month, DayOfMonth, Time, Offense_Description, District = DISTRICT, Race, \n    Ethnicity, Latitude = Lat, Longitude = Long, OfficerID = Officer_ID, data_type\n  )\n\n# Combined for ultimate Data coordination!\ncombined_wc = bind_rows(citations_processed, warnings_processed)\n\n# Add ultimate binary outcome! 0 = Citation, 1 = Warning/ Got out of ticket\ncombined_wc = combined_wc %&gt;%\n  mutate(\n    BinaryOutcome = ifelse(outcome == \"Warning\", 1,0)\n  )\n\n## Change to Title Case for District Names\ncombined_wc$District = tools::toTitleCase(tolower(combined_wc$District))\n\n## Examining Unverified data\n## After examination, unverified only makes up 0.0143 or 1.43% of the data set, so we will remove\n## because it is a very small portion of the total proportion. \ncombined_wc %&gt;%\n  count(District) %&gt;%\n  mutate(Proportion = n / sum(n)) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 11 × 3\n   District         n Proportion\n   &lt;chr&gt;        &lt;int&gt;      &lt;dbl&gt;\n 1 Sully        18612  0.208    \n 2 Springfield  12581  0.140    \n 3 Braddock     10292  0.115    \n 4 Franconia    10033  0.112    \n 5 Hunter Mill   8718  0.0972   \n 6 Mason         8168  0.0911   \n 7 Dranesville   7143  0.0797   \n 8 Providence    6713  0.0749   \n 9 Mount Vernon  6113  0.0682   \n10 Unverified    1281  0.0143   \n11 &lt;NA&gt;             1  0.0000112\n\n## Filter out Unverified and NA\ncombined_wc = combined_wc %&gt;%\n  filter(District != \"Unverified\")\n\ncombined_wc = combined_wc %&gt;%\n  filter(!is.na(District))\n\n## Filter out Other/Unknown Gender\ncombined_wc_mf = combined_wc %&gt;%\n  filter(Gender != \"Other/Unknown\")\n\n## Now for some visuals: Gender Chart\n## Examining the proportion of stops resulting in a Warning Vs Citation\n## the Warning rate is the proportion of incidents that are warnings.\ngender_warning_rate = combined_wc_mf %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(\n    Total_Incidents = n(),\n    Warning_Rate = mean(BinaryOutcome)\n  ) %&gt;%\n  ungroup()\n\ngender_chart = ggplot(gender_warning_rate,\n                      aes(x = Gender, y = Warning_Rate, fill = Gender)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(aes(label = scales::percent(Warning_Rate, accuracy = 0.1)),\n            vjust = -0.5, size = 5) +\n  scale_y_continuous(labels = scales::percent, limits = c(0, max(gender_warning_rate$Warning_Rate) * 1.1)) +\n  labs(\n    title = \"Warning Rate by Gender\",\n    subtitle = \"Proportion of stops resulting in a Warning (vs Citation)\",\n    x = \"Gender\",\n    y = \"Warning Rate\"\n  ) + theme_gray() + theme(plot.title = element_text(hjust = 0.5)) + theme(plot.subtitle = element_text(hjust = 0.5)) +\n  scale_fill_manual(values = c(\"Female\" = \"pink\", \"Male\" = \"skyblue\"))\n\ngender_chart\n\n\n\n\n\n\n\n\n\n## Now the Chi-Squared Test starting with the Contingency Table\ncontingency_tbl = combined_wc_mf %&gt;%\n  filter(Gender %in% c(\"Male\", \"Female\")) %&gt;%\n  select(Gender, BinaryOutcome) %&gt;%\n  table()\n\ncontingency_tbl\n\n        BinaryOutcome\nGender       0     1\n  Female 20478  8777\n  Male   43657 15408\n\nchi_sq_results = chisq.test(contingency_tbl)\n\nchi_sq_results\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  contingency_tbl\nX-squared = 150.62, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Nathania Stephens. I go by Nat. I am a graduate student at George Mason University. I am pursuing a MS in Data Analytics Engineering. Quite fitting, as I have always been passionate about learning and expanding my education in data discovery, literacy and coding. It is such an exciting time to learn with the growth of data and new technology such as Artificial Intelligence. In my career I have been a casual coder, finding opportunities to test out my skills when able. I hope to bring my enhanced knowledge and expanded coding skills back to the workplace to teach others and improve upon my organization’s objectives."
  },
  {
    "objectID": "Helpfulinfo.html",
    "href": "Helpfulinfo.html",
    "title": "Helpful R Info",
    "section": "",
    "text": "Is it a logical (Boolean), string, etc?\n\nx = 5\nclass(x)\n\n[1] \"numeric\"\n\n\nOr use is….?\n\nis.numeric(x)\n\n[1] TRUE\n\n\n\n\n\n\nx = as.character(x)\nx\n\n[1] \"5\"\n\n\n\nis.numeric(x)\n\n[1] FALSE\n\n\n\nclass(x)\n\n[1] \"character\""
  },
  {
    "objectID": "Helpfulinfo.html#identify-data-types",
    "href": "Helpfulinfo.html#identify-data-types",
    "title": "Helpful R Info",
    "section": "",
    "text": "Is it a logical (Boolean), string, etc?\n\nx = 5\nclass(x)\n\n[1] \"numeric\"\n\n\nOr use is….?\n\nis.numeric(x)\n\n[1] TRUE"
  },
  {
    "objectID": "Helpfulinfo.html#change-a-data-type",
    "href": "Helpfulinfo.html#change-a-data-type",
    "title": "Helpful R Info",
    "section": "",
    "text": "x = as.character(x)\nx\n\n[1] \"5\"\n\n\n\nis.numeric(x)\n\n[1] FALSE\n\n\n\nclass(x)\n\n[1] \"character\""
  },
  {
    "objectID": "Helpfulinfo.html#remove-a-variable",
    "href": "Helpfulinfo.html#remove-a-variable",
    "title": "Helpful R Info",
    "section": "Remove a Variable",
    "text": "Remove a Variable\n\nrm(b)\n\n#Vectors Collection of elements, like a list.\n\nmyvector = c('apple', 'banana', 'orange', 'pineapple')\nmyvector\n\n[1] \"apple\"     \"banana\"    \"orange\"    \"pineapple\""
  },
  {
    "objectID": "Helpfulinfo.html#get-current-directory",
    "href": "Helpfulinfo.html#get-current-directory",
    "title": "Helpful R Info",
    "section": "Get current directory",
    "text": "Get current directory\nGet the current working directory you are in.\n\ngetwd()"
  },
  {
    "objectID": "Helpfulinfo.html#change-directory",
    "href": "Helpfulinfo.html#change-directory",
    "title": "Helpful R Info",
    "section": "Change directory",
    "text": "Change directory\nChange your directory another location\n\nsetwd(\"EnTeR NeW PaTh\") # use // or \\ for windows OS"
  },
  {
    "objectID": "Helpfulinfo.html#view",
    "href": "Helpfulinfo.html#view",
    "title": "Helpful R Info",
    "section": "View",
    "text": "View\nTo see a data frame use “View”\n\nView(penguins)"
  },
  {
    "objectID": "Helpfulinfo.html#head",
    "href": "Helpfulinfo.html#head",
    "title": "Helpful R Info",
    "section": "Head",
    "text": "Head\nSee the first few rows of the data frame.\n\nhead(penguins)\n\n  species    island bill_len bill_dep flipper_len body_mass    sex year\n1  Adelie Torgersen     39.1     18.7         181      3750   male 2007\n2  Adelie Torgersen     39.5     17.4         186      3800 female 2007\n3  Adelie Torgersen     40.3     18.0         195      3250 female 2007\n4  Adelie Torgersen       NA       NA          NA        NA   &lt;NA&gt; 2007\n5  Adelie Torgersen     36.7     19.3         193      3450 female 2007\n6  Adelie Torgersen     39.3     20.6         190      3650   male 2007"
  },
  {
    "objectID": "Helpfulinfo.html#structure",
    "href": "Helpfulinfo.html#structure",
    "title": "Helpful R Info",
    "section": "Structure",
    "text": "Structure\n\nstr(penguins)\n\n'data.frame':   344 obs. of  8 variables:\n $ species    : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island     : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_len   : num  39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_dep   : num  18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_len: int  181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass  : int  3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex        : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year       : int  2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ..."
  },
  {
    "objectID": "Helpfulinfo.html#summary",
    "href": "Helpfulinfo.html#summary",
    "title": "Helpful R Info",
    "section": "Summary",
    "text": "Summary\nSummary will provide a summary of the rows, including NA rows.\n\nsummary(penguins)\n\n      species          island       bill_len        bill_dep    \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n  flipper_len      body_mass        sex           year     \n Min.   :172.0   Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0   1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0   Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9   Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0   3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0   Max.   :6300                Max.   :2009  \n NA's   :2       NA's   :2"
  },
  {
    "objectID": "Helpfulinfo.html#glimpse",
    "href": "Helpfulinfo.html#glimpse",
    "title": "Helpful R Info",
    "section": "Glimpse",
    "text": "Glimpse\nUse “glimpse” to see the number of rows, columns, and each of the column names with their data type.\n\nglimpse(penguins)"
  },
  {
    "objectID": "Helpfulinfo.html#check-for-nas",
    "href": "Helpfulinfo.html#check-for-nas",
    "title": "Helpful R Info",
    "section": "Check for NAs",
    "text": "Check for NAs\nCheck for NAs or not available/ missing values. Then determine what you will do with them. Will you get rid of them or insert a value? This only tells you yes and no there are NA values and how much but it wont tell you where those NAs are. However, summary above shows what columns they are in.x\n\ntable(is.na(penguins))\n\n\nFALSE  TRUE \n 2733    19"
  },
  {
    "objectID": "RedesignCode.html",
    "href": "RedesignCode.html",
    "title": "Redesign Code",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(leaflet)\nlibrary(tidyverse)\nlibrary(usmap)\nlibrary(sf)\n\nLinking to GEOS 3.13.1, GDAL 3.11.0, PROJ 9.6.0; sf_use_s2() is TRUE"
  },
  {
    "objectID": "RedesignCode.html#libraries",
    "href": "RedesignCode.html#libraries",
    "title": "Redesign Code",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(leaflet)\nlibrary(tidyverse)\nlibrary(usmap)\nlibrary(sf)\n\nLinking to GEOS 3.13.1, GDAL 3.11.0, PROJ 9.6.0; sf_use_s2() is TRUE"
  },
  {
    "objectID": "RedesignCode.html#default-class-theme",
    "href": "RedesignCode.html#default-class-theme",
    "title": "Redesign Code",
    "section": "Default Class Theme",
    "text": "Default Class Theme\n\nhw &lt;- theme_gray()+ theme(\n  plot.title=element_text(hjust=0.5),\n  plot.subtitle=element_text(hjust=0.5),\n  plot.caption=element_text(hjust=-.5),\n  \n  strip.text.y = element_blank(),\n  strip.background=element_rect(fill=rgb(.9,.95,1),\n                                colour=gray(.5), linewidth =.2),\n  \n  panel.border=element_rect(fill=FALSE,colour=gray(.70)),\n  panel.grid.minor.y = element_blank(),\n  panel.grid.minor.x = element_blank(),\n  panel.spacing.x = unit(0.10,\"cm\"),\n  panel.spacing.y = unit(0.05,\"cm\"),\n  \n  # axis.ticks.y= element_blank()\n  axis.ticks=element_blank(),\n  axis.text=element_text(colour=\"black\"),\n  axis.text.y=element_text(margin=margin(0,3,0,3)),\n  axis.text.x=element_text(margin=margin(-1,0,3,0))\n)"
  },
  {
    "objectID": "RedesignCode.html#redesign-1-bar-chart",
    "href": "RedesignCode.html#redesign-1-bar-chart",
    "title": "Redesign Code",
    "section": "Redesign 1: Bar chart",
    "text": "Redesign 1: Bar chart\n\nnps_sar = read.csv('NPmostSARincidents.csv')\n\nggplot(nps_sar, aes(x = reorder(NationalPark, NumberSARIncidents), y = NumberSARIncidents)) +\n  geom_bar(stat = 'identity', fill = 'forestgreen')  +\n  coord_flip() +\n  labs(\n    title = \"Number of Search and Rescue Incidents by Park\",\n    x = NULL,\n    y = \"Number of Incidents\") + hw"
  },
  {
    "objectID": "RedesignCode.html#redesign-1-chorophelt-map",
    "href": "RedesignCode.html#redesign-1-chorophelt-map",
    "title": "Redesign Code",
    "section": "Redesign 1: Chorophelt map",
    "text": "Redesign 1: Chorophelt map\n\nusa_tbl = map_data(\"state\") %&gt;% as_tibble()\n\n# To ensure the NPS SAR data matches, change state column to lowercase.\nnps_sar$State1 = tolower(nps_sar$State1)\n\n# Group by state and add totals\nlibrary(dplyr)\nsarbystate = nps_sar %&gt;%\n  group_by(State1) %&gt;%\n  summarize(\n    TotalbyState = sum(NumberSARIncidents, na.rm = TRUE)) %&gt;%\n  arrange(desc(TotalbyState))\n\n# Join the state map with the sar table\nusamapsar = left_join(usa_tbl, sarbystate, by = c('region' = 'State1'))\n\nmap1 = ggplot(usamapsar, aes( x = long, y =lat, group= group)) +\n  geom_polygon(aes(fill = TotalbyState), color = 'black') + \n  scale_fill_gradient(name = '# of Incidents', low = 'yellow',\n                      high = 'red', na.value = 'grey80') + \n  coord_map() +\n  theme_minimal() +\n  labs(\n    title = \"Top 11 States with Search and Rescue Incidents\", \n    x ='', \n    y = '',\n    caption = '*Gray states contain no data') + \n  theme(plot.title = element_text(size = 20))\n\nmap1"
  },
  {
    "objectID": "RedesignCode.html#redesign-1-interactive-map",
    "href": "RedesignCode.html#redesign-1-interactive-map",
    "title": "Redesign Code",
    "section": "Redesign 1: Interactive Map",
    "text": "Redesign 1: Interactive Map\n\nnpspoints = read.csv('NPSBoundary.csv')\n\ntopnps = nps_sar$NationalPark\n\ntopnps = c(topnps, 'Sequoia National Park', 'Kings Canyon National Park')\n\nfiltered_npspoints = npspoints %&gt;%\n  filter(UNIT_NAME %in% topnps)\n\nfiltered_npspoints$NumberSARIncidents = c(131,202,165,279,224,341,285,133,371,198,131,785,300,203,732,146,204)\n\nleaflet(filtered_npspoints)%&gt;% \n  addTiles() %&gt;% \n  addMarkers(lng = ~x,\n             lat = ~y,\n             label = ~paste0(UNIT_NAME,\"; Incidents: \", NumberSARIncidents))"
  },
  {
    "objectID": "RedesignCode.html#redesign-2-choropleth-map",
    "href": "RedesignCode.html#redesign-2-choropleth-map",
    "title": "Redesign Code",
    "section": "Redesign 2: Choropleth Map",
    "text": "Redesign 2: Choropleth Map\n\nstatesSAR = read.csv('StatesSAR.csv')\n\n# lower case states\nstatesSAR$State = tolower(statesSAR$State)\n\nstatesSARmap = left_join(usa_tbl, statesSAR, by = c('region' = 'State'))\n\nmapSAR = ggplot(statesSARmap, aes(x = long, y = lat, group = group)) +\n  geom_polygon(aes(fill = NumberSARIncidents), color = 'black') +\n  scale_fill_gradient(name = '# of SAR Incidents', low = 'yellow',\n                      high = 'red', na.value = 'grey80') +\n  coord_map() +\n  labs(\n    title = \"Top 20 States with the Most Search and Rescue Incidents\",\n    x = '',\n    y = '',\n    caption = '*Gray represents no data') +\n  theme(plot.title = element_text(size = 15)) + theme_minimal()\n\nmapSAR"
  },
  {
    "objectID": "RedesignCode.html#redesign-2-choropleth-map-with-alaska",
    "href": "RedesignCode.html#redesign-2-choropleth-map-with-alaska",
    "title": "Redesign Code",
    "section": "Redesign 2: Choropleth map with Alaska",
    "text": "Redesign 2: Choropleth map with Alaska\n\n# Use us_map to get all states but only take full and geom columns\nSARstates = read.csv('StatesSAR.csv')\nusa = us_map('states')\nusa_sub = usa[, c(\"full\", \"geom\")]\nusajoinsar = left_join(usa_sub, SARstates, by = c('full' = 'State'))\n\nggplot() +\n  geom_sf(data=usajoinsar, aes(fill= NumberSARIncidents), color = 'gray70') +\n  scale_fill_gradient(name = '# of SAR Incidents', low = 'yellow', high = 'red', na.value = 'grey30') +\n  labs(title = \"Top 20 States\", \n       subtitle = \"with the Most Search and Rescue Incidents\",\n       caption = c('Gray represents no data')) +\n  theme_void() +\n  theme(title = element_text(face = 'bold'),\n        legend.position = 'bottom')"
  }
]