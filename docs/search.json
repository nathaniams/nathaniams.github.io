[
  {
    "objectID": "FinalProject.html#group-1",
    "href": "FinalProject.html#group-1",
    "title": "Final Project",
    "section": "Group 1:",
    "text": "Group 1:\n\nHiba Awan\nNathania Stephens"
  },
  {
    "objectID": "FinalProject.html#motivation-purpose",
    "href": "FinalProject.html#motivation-purpose",
    "title": "Final Project",
    "section": "Motivation/ Purpose",
    "text": "Motivation/ Purpose\nIn 2023, there were over 30,000 arrests and close to 65,000 citations in Fairfax County. The Fairfax County boundaries, include areas such as Centreville, Chantilly, Herndon, Reston, Tysons Corner, McLean, Merrifield, George Mason, Annadale, Burke, Springfield, Alexandria, Lorton to name a few. If you live, work, or study in these areas then this project should be of interest to you. This project aims to inform Fairfax County patrons of crime information and hopefully provide some statistical insights that could be applicable."
  },
  {
    "objectID": "FinalProject.html#goals-objectives",
    "href": "FinalProject.html#goals-objectives",
    "title": "Final Project",
    "section": "Goals/ Objectives",
    "text": "Goals/ Objectives\nIn order to provide relevant and insightful crime information, several different visualization methods were applied to help easily interpret and compare data. Statistical learning techniques were utilized to help understand statistic significantly factors and associations between variables. Since the data utilized for this project is largely categorical the project focuses on techniques such as Chi-Squared Test, Logistic Regression, Decision Trees and Random Forest."
  },
  {
    "objectID": "FinalProject.html#overview",
    "href": "FinalProject.html#overview",
    "title": "Final Project",
    "section": "Overview",
    "text": "Overview\nThe Fairfax County Police Department (FCPD) operate in eight police districts and within those districts they are broken out into smaller Police Service Areas (PSAs). In 2023, the Fairfax County Police Department had more arrest and citation than the year prior. There were more assault offenses, theft and larceny. However, there were less homicide offenses, sex offenses, robbery, and fatal crashes. FCPD makes this information and its corresponding data available on their website (FCDP Open Data Portal, n.d). Three datasets were pulled from the Fairfax County Police Department website. They covered arrest, citations, and warnings in the year 2023. For simplicity, general definitions are provided:\n\nArrest - When a person is taken into custody to answer for an offense or when there is a deprivation or restraint of a person’s liberty in any significant way.\nCitation - Formal notice issued by law enforcement officer for a violation of law, typically related to traffic laws or other minor offenses. Typically requiring a violator to appear in court or pay a fine.\nWarning - When a violation, typically minor, has been made but an officer issues a warning rather than a citation.\n\nThe data sets included between 24 and 34 variables, but many of the variables were redundant or were not applicable to the research (e.g. web_address, phone_number, name).(FCDP Open Data Portal, n.d) Redundant columns and non-applicable columns were removed (Table 1). Additional columns were created from parsed information derived from original attributes, e.g. Hour of Day and Day of Month.\n\nTable 1: Attribute table\n\n\n\n\n\n\n\nColumn Name\nData Type\nDescription\n\n\n\n\nDate\nDate\nDate of Violation\n\n\nTime\nChr\nTime of Violation\n\n\nOffense\nChr\nDescription of Violation\n\n\nGender\nChr\nGender of Violator\n\n\nEthnicity\nChr\nHispanic or Non-Hispanic\n\n\nDistrict\nChr\nAdministrative area\n\n\nLatitude\nDbl\nCoordinates measuring north/ south of equator\n\n\nLongitude\nDbl\nCoordinates measuring east/ west of prime meridian\n\n\nOutcome\nChr\nResult of violation, arrest, citation, or warning"
  },
  {
    "objectID": "FinalProject.html#about-the-data",
    "href": "FinalProject.html#about-the-data",
    "title": "Final Project",
    "section": "About the Data",
    "text": "About the Data\nThree datasets were pulled from the Fairfax County Police Department website. They covered arrest, citations, and warnings in the year 2023. For simplicity, general definitions are provided:\n\nArrest - When a person is taken into custody to answer for an offense or when there is a deprivation or restraint of a person’s liberty in any significant way.\nCitation - Formal notice issued by law enforcement officer for a violation of law, typically related to traffic laws or other minor offenses. Typically requiring a violator to appear in court or pay a fine.\nWarning - When a violation, typically minor, has been made but an officer issues a warning rather than a citation.\n\nThe data sets included between 24 and 34 variables, but some of many of the variables were redundant or were not applicable to the research (e.g. web_address, phone_number, name). Redundant columns and non-applicable columns were removed (Table 1).\n\nTable 1: Attribute table\n\n\n\n\n\n\n\nColumn Name\nData Type\nDescription\n\n\n\n\nDate\nDate\nDate of Violation\n\n\nTime\nChr\nTime of Violation\n\n\nOffense\nChr\nDescription of Violation\n\n\nGender\nChr\nGender of Violator\n\n\nEthnicity\nChr\nHispanic or Non-Hispanic\n\n\nDistrict\nChr\nAdministrative area\n\n\nLatitude\nDbl\nCoordinates measuring north/ south of equator\n\n\nLongitude\nDbl\nCoordinates measuring east/ west of prime meridian\n\n\nOutcome\nChr\nResult of violation, arrest, citation, or warning"
  },
  {
    "objectID": "FinalProject.html#limitations-and-assumptions",
    "href": "FinalProject.html#limitations-and-assumptions",
    "title": "Final Project",
    "section": "Limitations and Assumptions",
    "text": "Limitations and Assumptions\nDue to the nature of the data available from the Fairfax County Police Department website, analysis was limited to qualitative techniques. The approach taken for the project focused on predicting through qualitative responses or classification. This means that each record pulled from the Fairfax County Police Department (FCPD) would be assigned to a category or class.\nWhile understanding local crime is the goal of this project, the data acquired only accounts for crime that was recorded by FCPD. It does not take into account crimes that were not report or any other crime that was not reported through FCPD channels."
  },
  {
    "objectID": "FinalProject.html#cleaning-and-transformation",
    "href": "FinalProject.html#cleaning-and-transformation",
    "title": "Final Project",
    "section": "Cleaning and Transformation",
    "text": "Cleaning and Transformation\nIn cases where applicable, duplicate and null values were removed. To address questions related to gender, the data needed to be standardized and correctly categorized. Column names needed to be consistent across the three datasets to merge. Gender was used over Sex. Next the column data would be transformed to consistent labels, e.g. Male, Female, and Other/Unknown. Records that included unverified or unknown were also removed in cases where they did not contribute to the overall analysis. To ensure these removals would not corrupt the analysis, proportions were examined to ensure its removal would not effect the overall results, typically less than 2% proportion. Depending on the research question, determined which datasets were merged."
  },
  {
    "objectID": "FinalProject.html#research-analysis",
    "href": "FinalProject.html#research-analysis",
    "title": "Final Project",
    "section": "Research & Analysis",
    "text": "Research & Analysis\n\nQuestion 1: Is there an association between gender and warnings?\nTo address this question the null and alternative hypothesis are established.\nNull Hypothesis: There is no association between gender and violation outcome, warning or citation. This would mean that the likelihood of a violator getting a warning is independent of gender.\nAlternative Hypothesis: There is an association between gender and violation outcome. This implies that gender affects the outcome of whether a violator is given a citation or warning.\nAccording to the cleaned and combined dataset for warnings and citation, there was a total of 88,320 records. By looking at the counts for each outcome (citation or warning), there are a lot more citations than there are warnings given out by FCPD. This stacked bar chart also shows that males have a higher count for both categories.\n\n\n\n\n\n\n\n\n\nNext, the warning rate for gender is calculated. This looks at the probability of a male or female violator receiving a Warning instead of a citation e.g. getting out of a ticket. To calculate warning rate, the number of warnings are divided by the total number of incidents.\n\\[\n\\begin{align*}\nWarning Rate = \\frac{\\text{Number of Warnings}}{\\text{Total Incidents (Warnings + Citations)}}\n\\end{align*}\n\\]\nThis shows a slight difference in proportion between the two genders, with females having a higher warning rate than males. In other words, females received more warnings than males. Is this difference significant or is it a result of chance or other factors? To help understand these results, the Chi-Square Test of Independence is used. The Chi-Square Test of Independence will help determine whether the variables, gender and outcome, are independent or if there is a relationship between them.\n\n\n\n\n\n\n\n\n\nTo implement the Chi-Square Test, a contingency table is generated, which shows the distribution for gender and outcome.\n\\[\n\\chi^2 = \\sum \\frac{(O-E)^2}{E}\n\\]\n\n\n\nGender\nCitations\nWarnings\n\n\n\n\nFemale\n20,478\n8,777\n\n\nMale\n43,657\n15,408\n\n\n\nThe results of the Chi-Square test shows:\n\nChi-Square Statistic (x-squared): The chi-square test statistic is 150.62. This is the discrepancy between the observed frequencies, citations and warnings, and the expected frequencies if there were no association between the gender and outcome. This is demonstrated in the below tables.\n\n\n\n\n\n\n\n\n\n\n\nGender\nExpected Citations\nObserved Citations\nExpected Warnings\nObserved Warnings\n\n\n\n\nFemale\n21,243\n20,478\n8,011\n8,777\n\n\nMale\n42,891\n43,657\n16,173\n15,408\n\n\n\n\nDegrees of Freedom (df): The degree of freedom for this test is 1, which is the number of rows minus 1 multiplied by number of columns minus 1.\np-value: The p-value is 2.2e-16 which is much smaller than 0.05. This represents the probability of observing the chi-square statistic, 150.62, or more if the null hypothesis were true.\n\nTo illustrate this, a heatmap is generated showing each values contribution to the chi-square test. This easily demonstrates that female and warning had the highest percentage.\n\n\n\n\n\n\n\n\n\nThus the null hypothesis is rejected. The results show that there is a statistically significant association between gender and violation outcome in Fairfax County. In other words, the Chi-square test indicates that the likelihood of a violation outcome is significantly associated with gender.\n\n\nQuestion 2: How Does Time and Location Factor into the Odds of a Citations?\nFor this research question, variables such as time of day, progression of week, day of month, and district are examined to determine if those variable influence an officer’s decision to issue a warning or citation. For this question, arrests are excluded since it is assumed a higher threshold of danger is imminent and would allow for less use of officers’ discretion.\nFirst, to explore the likelihood of getting a citation versus a warning (outcome of a violation), the citation rate is calculated, using the following equation.\n\\[\n\\begin{align*}\nCitation Rate = \\frac{\\text{Number of Citations}}{\\text{Total Incidents (Warnings + Citations)}}\n\\end{align*}\n\\]\nThen, a heatmap is generated to see if there are any patterns that would indicate trends over hours of the day and throughout the month.\n\n\n\n\n\n\n\n\n\nThe higher intensity or darker color areas represent a higher citation rate. During the hours of 0500-0600 (24-hour clock) the citation rate is much higher across most days of the month. This would imply that officers are less likely to issue warnings between 5am and 6am. This time also aligns with morning rush hour and higher volumes of traffic are expected. However, the same pattern of higher citation rates are not observed during evening rush hour. The heatmap also does not show higher intensities during the end of the month. Which is what we would expect to see if officers were attempting to meet monthly ticket quotas at the end of the month.\nTo visualize citation and warnings geospatially, a map is generated using the latitude and longitude associated to each record. While this map is helpful for understanding concentrations of citations and warning. It is misleading in show the most dominate outcomes. This is likely due to the quantity of data points overlaid on one another; however, since it is interactive areas can be zoomed in on for further insight.\n\n\n\n\n\n\nNext, through the use of Logistic Regression, the odds ratios is examined between citations and warnings for Fairfax county. Predictor variables, time of day, week progression, day of month, district are used to predict the probability that a warning or citation will occur. The results provide an odd ratios at the intercept of 0.532 which would indicate that the odds of getting a warning are less than 50/50. In other words, if pulled over for a traffic violation, it is not a flip of a coin whether you get a ticket or not. Your chance of getting a citation is higher according to the intercept generated through this logistic regression model. Additionally, the model showed factors that did increased the likelihood of getting a warning occurred in certain districts (Mount Vernon, Mason, and Franconia) and progression of the week influences leniency. In weekly progression, an odds ratio of 1.595 shows that as the week progresses from Monday to Sunday, the odds of recieving a warning increase by 59.5 %. Alternatively, factors such as time of day and day of the month showed an increased likelihood of a citation. Using the below graph, the top five odds ratio for citation and bottom five odds ratio for warnings are depicted.\n\n\n\n\n\n\n\n\n\nGender and race were originally included but removed to isolate only time and space variables. Additionally, after running both models, the Area Under the Curve (AUC) increased only slightly (+ 0.0004) after removal of race and gender. This would mean that while gender and race were statistically significant, they were not as strong of predictors as date/time and district when classifying an outcome of either citation or warning. The AUC for the model that focused date/time and district was 0.6289. Which means that 63% of the citation/warning outcome is predictable using day of the month, time of day, progression of week, and district. The other 37% of the outcome is due to other factors not captured in this model. These factors could include offense type, violator’s behavior, and officer variables. While not a particularly strong predictive model, the model helps understand the effects of time and location.\n\n\nQuestion 3: How do enforcement outcomes vary across districts and demographic groups?\nFinally, we’ll be exploring the relationships between the outcomes of crime incidents and their district and demographic (gender, race, ethnicity) variables. Addressing this final question involved combining the three data sets of different crime outcomes: arrest data, citation data, and warning data.\n(EnforcementOutcomesByDistrict)\nStarting with the outcomes across different districts, we start off by performing a chi-square test to establish whether there’s a relationship between the crime outcomes and the districts they took place in.\nNull Hypothesis: Enforcement outcome is independent of police district.\nAlternative hypothesis: Enforcement outcome and police district are associated.\nFor the chi-square implementation, a contingency table for the district and outcome counts is generated: (table)\nFrom both the visualization above and the table output, we can see that citations are the most common outcome among the districts. However, this won’t affect the test as chi-square allows unequal margin totals. The other outcomes, arrests and warnings, vary substantially in volume across districts. Some districts (like Sully, Springfield, Providence) show particularly high citation counts, while others (Braddock, Dranesville) show lower overall enforcement volumes.\nThen the test is performed:\n(Chi-Squared test)\nThe chi-square statistic is extremely large, which indicates a substantial deviation between observed and expected counts. The p-value is also low enough that we reject the null hypothesis. Therefore, there is a statistically significant association between district and enforcement outcome.\nFrom this, we can see that enforcement severity varies by district and district-level patterns justify further modeling.\n(Enforcement Outcomes by Demographics)\n(results) Performing multinomial logistic regression shows that enforcement outcomes vary systematically across both police districts and demographic groups. Certain districts, such as Providence and Sully, have higher relative odds of arrest compared to citations or warnings. Race and ethnicity are also significant predictors: Black and Hispanic individuals are more likely to be arrested rather than receive less severe enforcement actions. Gender has smaller but detectable effects. These results indicate that enforcement outcomes are influenced by both location and demographic characteristics."
  },
  {
    "objectID": "Resources.html",
    "href": "Resources.html",
    "title": "Helpful Links",
    "section": "",
    "text": "Quarto websites https://quarto.org/docs/websites."
  },
  {
    "objectID": "Resources.html#helpful-sites",
    "href": "Resources.html#helpful-sites",
    "title": "Helpful Links",
    "section": "",
    "text": "Quarto websites https://quarto.org/docs/websites."
  },
  {
    "objectID": "Redesign.html",
    "href": "Redesign.html",
    "title": "Redesign Project",
    "section": "",
    "text": "For this project, an Outforia 2022 article on “The USA’s Search and Rescue Hotspots,” was chosen. Within the article there are four tables consisting of the highest numbers of Search and Rescue (SAR) incidents grouped by either National Parks or States. [1] For this project, two of the four tables were chosen for a redesign.\nThe article focuses on Search and Rescue incidents that occurred between 2018-2020 within US National Parks. Considering the context and the author’s statement at the bottom of the article, this article and tables were likely created for readers who enjoy the outdoors, hiking, exploring and soaking in the beauty of national parks. It is also likely that the article is intended to help novice outdoor adventurers by providing awareness into the highest number of search and rescue events.This helps readers make informed decisions regarding their next outdoor adventure!\nWhile the article is informative and helps readers understand which US National Parks could be considered most dangerous, the goal of this project was to enhance the visualization through best practices and improved graph design using R.\nThe first table provides the top 16 national parks with the highest number of incidents between 2018-2020. Some of the disadvantages of this table is that it is a table, while simple and easy to read, it does not provide quick comparison like a bar chart would offer. Another consideration is the unnecessary visual at the top of the table. Above the table are the three parks with the highest number of incidents. The only additional information provided are the states the parks reside in; however, this is easily overlooked based on their position, color, and pretty yet distracting pictures.\n\n\n\n\n\n\n\nNPS Most Search and Rescues by Outforia\n\n\nThe second table provides the top 20 US States with the most search and rescue incidents. Yes, Virginia made the list. In this table we have more information on total search and rescue incidents, but grouped by State instead of individual parks. This provides a holistic view of incidents across a given state. Again we are given a table, which doesn’t offer fast comparison and the top three states which offer no additional information from the table but does offer pictures of the states’ flags and an outline of the state.\n\n\n\n\n\n\n\nStates Most Search and Rescue by Outforia"
  },
  {
    "objectID": "Redesign.html#the-original-design",
    "href": "Redesign.html#the-original-design",
    "title": "Redesign Project",
    "section": "",
    "text": "For this project, an Outforia 2022 article on “The USA’s Search and Rescue Hotspots,” was chosen. Within the article there are four tables consisting of the highest numbers of Search and Rescue (SAR) incidents grouped by either National Parks or States. [1] For this project, two of the four tables were chosen for a redesign.\nThe article focuses on Search and Rescue incidents that occurred between 2018-2020 within US National Parks. Considering the context and the author’s statement at the bottom of the article, this article and tables were likely created for readers who enjoy the outdoors, hiking, exploring and soaking in the beauty of national parks. It is also likely that the article is intended to help novice outdoor adventurers by providing awareness into the highest number of search and rescue events.This helps readers make informed decisions regarding their next outdoor adventure!\nWhile the article is informative and helps readers understand which US National Parks could be considered most dangerous, the goal of this project was to enhance the visualization through best practices and improved graph design using R.\nThe first table provides the top 16 national parks with the highest number of incidents between 2018-2020. Some of the disadvantages of this table is that it is a table, while simple and easy to read, it does not provide quick comparison like a bar chart would offer. Another consideration is the unnecessary visual at the top of the table. Above the table are the three parks with the highest number of incidents. The only additional information provided are the states the parks reside in; however, this is easily overlooked based on their position, color, and pretty yet distracting pictures.\n\n\n\n\n\n\n\nNPS Most Search and Rescues by Outforia\n\n\nThe second table provides the top 20 US States with the most search and rescue incidents. Yes, Virginia made the list. In this table we have more information on total search and rescue incidents, but grouped by State instead of individual parks. This provides a holistic view of incidents across a given state. Again we are given a table, which doesn’t offer fast comparison and the top three states which offer no additional information from the table but does offer pictures of the states’ flags and an outline of the state.\n\n\n\n\n\n\n\nStates Most Search and Rescue by Outforia"
  },
  {
    "objectID": "Redesign.html#about-the-data",
    "href": "Redesign.html#about-the-data",
    "title": "Redesign Project",
    "section": "About the data",
    "text": "About the data\nTo obtain the data Outforia submitted a freedom of information request to the National Park Authority for search and rescue incidents between 2018-2020 for all US national parks. In cases where national parks spanned more than one state, Outforia counted the number of incidents for each state. [1] In this redesign project, data from the tables were used.\nInformation from National Park Services (NPS) site was used to supplement additional information. [2] For example, adding state to the national parks data and adding coordinates to allow for plotting on the interactive map."
  },
  {
    "objectID": "Redesign.html#purpose-of-redesign",
    "href": "Redesign.html#purpose-of-redesign",
    "title": "Redesign Project",
    "section": "Purpose of Redesign",
    "text": "Purpose of Redesign\nThe objectives of redesigning these tables are to provide informative graphs to help readers further interpret the data. By ensuring the graphs focus on accurate comparisons, simplified and uncluttered graphs, and appropriate context to keep the readers engaged. This will help readers make informed decisions about their next visit to a National Park.\n\nRedesign of Table 1\nIn this redesign, a horizontal bar chart is used. This allows for easy reading of the park names while still arranging the number of incidents from high to low. Also, this chart provides quick comparison for readers to easily compare bar lengths. In the book, Storytelling with data, “Our eyes compare the end points of the bars, so it is easy to see quickly which category is the biggest, which is the smallest, and also the incremental difference between categories.” [3]\n\n\n\n\n\n\n\n\n\n\n\nRedesign of Table 1, another look\nIn this redesign, table 1 is transformed to provide a map view of the data. This takes a different approach to understanding the search and rescue incidents from table 1. Here we are still using the same data from table 1, but instead of representing the park the state is used instead to represent states with the most incidents. This is another way to visualize the information from table 1. Another consideration is that some National Parks span more than one state. In this map, only one state was selected for each National Park. If data was available for each of the search and rescue events, then we could plot those events and have a more exact number of what state the incidents occurred in.\n\n\n\n\n\n\n\n\n\n\n\nRedesign of Table 1, a better look\nHere is another way to visualize the information that blends location while still providing the number of Search and Rescue Incidents for the top parks. The use of an interactive map is to allow readers not only to understand the parks with the highest number of incidents, but to help explore where those parks are and how large they area. The interactive map allows the reader to zoom in and out and pan around the map. Each marker has the name of the Park and the number of incidents. *Note: Sequoia National Park and Kings Canyon National Park were originally combined in the table. However, the NPS has them as separate parks.Therefore the number of incidents were divide between the two parks.\n\n\n\n\n\n\n\n\nRedesign of Table 2\nIn table 2 we see 20 states listed with the total number of Search and Rescue incidents from 2018-2020. Here it seems like the visual is on the right track; however, after counting the states there are only 19 highlighted! Oh yes, sneaky Alaska and Hawaii are not represented in this map.\n\n\n\n\n\n\n\n\n\nIn order to get the two states in the chart, without zooming too far out and losing the state-level resolution, another approach to the map view must be taken using library(usmap) and library(sf). Now we have all top 20 states properly represented on the map. To reduce clutter and unnecessary information [3], the x-axis and y-axis were removed. This is not only good practice to reduce clutter but another consideration is the placement of Alaska, Hawaii, and Puerto Rico. They are placed toward the bottom of the map view to include them in the graph not to represent their actual locations.\n\n\n\n\n\n\n\n\n\nInitially, an unusual error occurred when trying to render from R Studio to html. The map was missing the data overlay. The map generated without issues in R Studio. For a workaround, I saved the ggplot as an image and included in the qmd. After revisiting this issue on the Redesign Code page, the issue was discovered in the variable name for the dataframe that was being used. After renaming the variable the issue was resolved."
  },
  {
    "objectID": "Redesign.html#video-presentation",
    "href": "Redesign.html#video-presentation",
    "title": "Redesign Project",
    "section": "Video Presentation",
    "text": "Video Presentation\nThis is a test."
  },
  {
    "objectID": "Redesign.html#references",
    "href": "Redesign.html#references",
    "title": "Redesign Project",
    "section": "References",
    "text": "References\n\n[1]\nBorg, C., 2022, “The USA’s Search And Rescue Hotspots,” Outforia. [Online]. Available: https://outforia.com/search-and-rescue-hotspots/\n\n\n[2]\n2024, “NPS – Land Resources Division Boundary and Tract Data Service,” National Park Service. [Online]. Available: https://public-nps.opendata.arcgis.com/maps/c8d60ffcbf5c4030a17762fe10e81c6a/about\n\n\n[3]\nKnaflic, C. N., 2015. Storytelling with data: a data visualization guide for business professionals (1st ed.). Wiley."
  },
  {
    "objectID": "FinalProjectCode.html",
    "href": "FinalProjectCode.html",
    "title": "Final Project Code",
    "section": "",
    "text": "Final Project Code\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(leaflet)\nlibrary(usmap)\nlibrary(sf)\nlibrary(readr)\nlibrary(scales)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(pheatmap)\n\n# Read in the raw data for Warnings and Citations\nwarnings = read_csv(\"2023_warning_data.csv\", \n                         col_types = cols(Warnings_Date = col_date(format = \"%m/%d/%Y\"), \n                                          WEB_ADDRESS = col_skip(), PHONE_NUMBER = col_skip(), \n                                          NAME = col_skip()))\n\ncitations = read_csv(\"2023_citation_data.csv\", \n                                col_types = cols(Date = col_date(format = \"%m/%d/%Y\"), \n                                                 WEB_ADDRESS = col_skip(), PHONE_NUMBER = col_skip(), \n                                                 NAME = col_skip()))\n\n# Rename some columns\ncitations = citations %&gt;%\n  rename(ViolationDate = Date)\n\n# change Gender to sex in warnings and change date column name\nwarnings = warnings %&gt;%\n  rename(Sex = Gender)\n\nwarnings = warnings %&gt;%\n  rename(ViolationDate = Warnings_Date)\n\n# Adjust Citations and prepare for Merge\n# Assumption that ID is the officer's ID\ncitations_processed = citations %&gt;%\n  mutate(\n    outcome = \"Citation\",\n    Gender = case_when(\n      Sex == \"M\" ~ \"Male\",\n      Sex == \"F\" ~ \"Female\",\n      TRUE ~ \"Other/Unknown\"\n    ),\n    Year = year(ViolationDate),\n    Month = month(ViolationDate),\n    DayOfMonth = day(ViolationDate),\n    Time = parse_date_time(Time, \"HM\"),\n    data_type = \"Citation\"\n  ) %&gt;%\n  select(\n    outcome, Gender, Year, Month, DayOfMonth, ViolationDate, Time, Offense_Description = Charge,\n    District = DISTRICT, Race, Ethnicity, Latitude, Longitude, OfficerID = ID, data_type\n  )\n\n# Adjust Warnings and prepare for Merge\nwarnings_processed = warnings %&gt;%\n  mutate(\n    outcome = \"Warning\",\n    Gender = case_when(\n      Sex == \"M\" ~ \"Male\",\n      Sex == \"F\" ~ \"Female\",\n      TRUE ~ \"Other/Unknown\"\n    ),\n    Year = year(ViolationDate),\n    Month = month(ViolationDate),\n    DayOfMonth = day(ViolationDate),\n    Time = parse_date_time(Time, \"HM\"),\n    data_type = \"Warning\"\n  ) %&gt;%\n  select(\n    outcome, Gender, Year, Month, DayOfMonth, ViolationDate, Time, Offense_Description, District = DISTRICT, Race, \n    Ethnicity, Latitude = Lat, Longitude = Long, OfficerID = Officer_ID, data_type\n  )\n\n# Combined for ultimate Data coordination!\ncombined_wc = bind_rows(citations_processed, warnings_processed)\n\n# Add ultimate binary outcome! 0 = Citation, 1 = Warning/ Got out of ticket\ncombined_wc = combined_wc %&gt;%\n  mutate(\n    BinaryOutcome = ifelse(outcome == \"Warning\", 1,0)\n  )\n\n## Change to Title Case for District Names\ncombined_wc$District = tools::toTitleCase(tolower(combined_wc$District))\n\n## Examining Unverified data\n## After examination, unverified only makes up 0.0143 or 1.43% of the data set, so we will remove\n## because it is a very small portion of the total proportion. \n# combined_wc %&gt;%\n#  count(District) %&gt;%\n#  mutate(Proportion = n / sum(n)) %&gt;%\n#  arrange(desc(n))\n\n## Filter out Unverified and NA\ncombined_wc = combined_wc %&gt;%\n  filter(District != \"Unverified\")\n\ncombined_wc = combined_wc %&gt;%\n  filter(!is.na(District))\n\n## Filter out Other/Unknown Gender\ncombined_wc_mf = combined_wc %&gt;%\n  filter(Gender != \"Other/Unknown\")\n\n\n## Stacked Bar chart for Outcome and Gender\nggplot(combined_wc_mf, aes(x = outcome, fill = Gender)) +\n  geom_bar() +\n  labs(\n    title = \"Count of Outcomes by Gender\",\n    x = \"Outcome Type\",\n    y = \"# of Incident\",\n    fill = \"Gender\"\n  ) + theme_gray() + theme(plot.title = element_text(hjust = 0.5)) + \n  scale_fill_manual(values = c(\"Female\" = \"lightpink2\", \"Male\" = \"lightsteelblue\"))\n\n\n\n\n\n\n\n\n\n## Now for some visuals: Gender Chart\n## Examining the proportion of stops resulting in a Warning Vs Citation\n## the Warning rate is the proportion of incidents that are warnings.\ngender_warning_rate = combined_wc_mf %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(\n    Total_Incidents = n(),\n    Warning_Rate = mean(BinaryOutcome)\n  ) %&gt;%\n  ungroup()\n\ngender_chart = ggplot(gender_warning_rate,\n                      aes(x = Gender, y = Warning_Rate, fill = Gender)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(aes(label = scales::percent(Warning_Rate, accuracy = 0.1)),\n            vjust = -0.5, size = 5) +\n  scale_y_continuous(labels = scales::percent, limits = c(0, max(gender_warning_rate$Warning_Rate) * 1.1)) +\n  labs(\n    title = \"Warning Rate by Gender\",\n    subtitle = \"Proportion of stops resulting in a Warning (vs Citation)\",\n    x = \"Gender\",\n    y = \"Warning Rate\"\n  ) + theme_gray() + theme(plot.title = element_text(hjust = 0.5)) + theme(plot.subtitle = element_text(hjust = 0.5)) +\n  scale_fill_manual(values = c(\"Female\" = \"lightpink2\", \"Male\" = \"lightsteelblue\"))\n\ngender_chart\n\n\n\n\n\n\n\n\n\n## Now the Chi-Squared Test starting with the Contingency Table\ncontingency_tbl = combined_wc_mf %&gt;%\n  filter(Gender %in% c(\"Male\", \"Female\")) %&gt;%\n  select(Gender, BinaryOutcome) %&gt;%\n  table()\n\nchi_sq_results = chisq.test(contingency_tbl)\n\nexpected_counts = chi_sq_results$expected\n\n\n# Heatmap\nlibrary(pheatmap)\n# observed count\nobserved_counts = chi_sq_results$observed\n# Calculated expected above.\n# Calculate residuals\npearson_residuals = chi_sq_results$residuals\n\n# Calculate contributions to chi-square statistic\ncontributions = (observed_counts - expected_counts)^2 / expected_counts\n\ntotal_chi_square = chi_sq_results$statistic\n\npercentage_contributions = 100 * contributions / total_chi_square\ncolnames(percentage_contributions) = c(\"Citation\", \"Warning\")\npheatmap(percentage_contributions,\n         display_numbers = TRUE,\n         cluster_rows = FALSE,\n         cluster_cols = FALSE,\n         main = \"Percentage Contribution to Chi-Square Statistic\",\n         fontsize = 11,\n         fontsize_row = 11,\n         fontsize_col = 11,\n         fontsize_number = 15\n         )\n\n\n\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(viridis)\n\nWarning: package 'viridis' was built under R version 4.5.2\n\n\nLoading required package: viridisLite\n\n\n\nAttaching package: 'viridis'\n\n\nThe following object is masked from 'package:scales':\n\n    viridis_pal\n\nlibrary(tidyr)\n\ncombined_wc_mf = combined_wc_mf %&gt;%\n  mutate(\n    Hour = hour(Time),\n    Minute = minute(Time),\n    Time_of_day = case_when(\n      Hour &gt;= 5 & Hour &lt; 10 ~ \"Morning Rush (0500-1000)\",\n      Hour &gt;= 10 & Hour &lt; 15 ~ \"Daytime (1000-1500)\",\n      Hour &gt;= 15 & Hour &lt; 19 ~ \"Evening Rush (1500-1900)\",\n      TRUE ~ \"Night/Late Night (1900-0500)\"\n    ) \n  )\n\nstop_count_by_day = combined_wc_mf %&gt;%\n  group_by(DayOfMonth) %&gt;%\n  summarise(Stop_Count = n()) %&gt;%\n  ungroup()\n\n# Make interactive heatmap using plotly\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nheatmap_days = combined_wc_mf %&gt;%\n  group_by(DayOfMonth, Hour) %&gt;%\n  summarise(\n    TotalOutcomes = n(),\n    TotalCitations = sum(BinaryOutcome == 0, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    Citation_Rate = TotalCitations/TotalOutcomes\n  ) %&gt;%\n  tidyr::complete(DayOfMonth, Hour, fill = list(TotalOutcomes = 0, Citation_Rate = 0))\n\n`summarise()` has grouped output by 'DayOfMonth'. You can override using the\n`.groups` argument.\n\n# label for x-axis\nall_hours = as.character(0:23)\n\n# Convert to factor\nheatmap_days$Hour = factor(heatmap_days$Hour)\nheatmap_days$DayOfMonth = factor(heatmap_days$DayOfMonth)\n\n# Text for interactive part\nheatmap_days = heatmap_days %&gt;%\n  mutate(text = paste0(\"Hour: \", Hour, \"\\n\", \"Day: \", DayOfMonth, \"\\n\", \"Citation Rate: \",round(Citation_Rate,2)))\n\n## trying to get all axes\nplot_heatmap_discrete_axes = ggplot(\n  heatmap_days, \n  aes(x = Hour, y = DayOfMonth, fill = Citation_Rate, text = text)\n) +\n  geom_tile(aes(width = 1, height = 1), color = \"white\", linewidth = 0.1) +\n  scale_fill_viridis(\n    option = \"magma\",\n    direction = -1,\n    name = \"Citation Rate\",\n    labels = scales::percent\n  ) +\n  scale_x_discrete(name = \"Hour of Day (0 = Midnight, 12 = Noon)\", drop = FALSE) + \n  scale_y_discrete(drop = FALSE) + \n  labs(\n    title = \"Citation Rate by Day of Month and Hour of Day\",\n    subtitle = \"Higher intensity (darker color) indicates a higher proportion of Citations\",\n    y = \"Day of the Month\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    axis.text.x = element_text(vjust = 0.1, hjust = 0.1),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\nggplotly(plot_heatmap_discrete_axes, tooltip = \"text\")\n\n\n\n\n\n\nplot_heatmap_discrete_axes = ggplot(\n  heatmap_days, \n  aes(x = Hour, y = DayOfMonth, fill = Citation_Rate, text = text)\n) +\n  geom_tile(aes(width = 1, height = 1), color = \"white\", linewidth = 0.1) +\n  scale_fill_viridis(\n    option = \"magma\",\n    direction = -1,\n    name = \"Citation Rate\",\n    labels = scales::percent\n  ) +\n  scale_x_discrete(name = \"Hour of Day (0 = Midnight, 12 = Noon)\", drop = FALSE) + \n  scale_y_discrete(drop = FALSE) + \n  labs(\n    title = \"Citation Rate by Day of Month and Hour of Day\",\n    subtitle = \"Higher intensity (darker color) indicates a higher proportion of Citations\",\n    y = \"Day of the Month\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    axis.text.x = element_text(vjust = 0.1, hjust = 0.1),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\nplot_heatmap_discrete_axes\n\n\n\n\n\n\n\n\n\nArrest2023 &lt;- read_csv(\"2023_arrest_data.csv\", \n                       col_types = cols(ArresteeNumber = col_skip(), \n                                        ArrestDate = col_date(format = \"%m/%d/%Y\"), \n                                        ArrestTime = col_character(), WEB_ADDRESS = col_skip(), \n                                        PHONE_NUMBER = col_skip(), NAME = col_skip()))\n\n## Remove any outliers by setting min/max lat and long.\nmin_lat = 38.6\nmax_lat = 39.0\nmin_lon = -77.6\nmax_lon = -77.0\n\n## Using all data but excluding any coordinates that are not within coordinates\narrest_data_clean = Arrest2023 %&gt;%\n  filter(\n    Latitude &gt;= min_lat,\n    Latitude &lt;= max_lat,\n    Longitude &gt;= min_lon,\n    Longitude &lt;= max_lon\n  )\n\n## Include district boundaries by reading in shape file downloaded from Fairfax County site\ndistrict_boundaries = st_read(\"Supervisor_Districts/Supervisor_Districts.shp\")\n\nReading layer `Supervisor_Districts' from data source \n  `C:\\Users\\Nat\\OneDrive - George Mason University - O365 Production\\Git\\nathaniams.github.io\\Supervisor_Districts\\Supervisor_Districts.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 9 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 11757190 ymin: 6905421 xmax: 11899000 ymax: 7070364\nProjected CRS: NAD83 / Virginia North (ftUS)\n\n## Transform to WGS84 projection to match basemap within leaflet\nif (st_crs(district_boundaries)$epsg != 4326) {\n  district_boundaries = st_transform(district_boundaries, 4326)\n}\n\n\npal = colorFactor(\n  palette = c(\"red\", \"gold1\"), \n  domain = combined_wc_mf$outcome, \n  levels = c(\"Citation\", \"Warning\") \n)\n\nleaflet() %&gt;%\n  addProviderTiles(providers$OpenStreetMap) %&gt;%\n  addPolygons(\n    data = district_boundaries,\n    fillOpacity = 0.5,\n    color = \"black\",\n    weight = 1,\n    popup = ~paste(\"District:\", DISTRICT)\n  ) %&gt;%\n  addCircleMarkers(\n    data = combined_wc_mf,\n    lng = ~Longitude,\n    lat = ~Latitude,\n    radius = 2.5,\n    color = ~pal(outcome),\n    stroke = FALSE,\n    fillOpacity = 0.4,\n    popup = ~paste(\n      \"Outcome: \", outcome, \"&lt;br&gt;\")\n  )\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(caTools)\n\nWarning: package 'caTools' was built under R version 4.5.2\n\nlibrary(pROC) \n\nWarning: package 'pROC' was built under R version 4.5.2\n\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n#Add HourOfDay\ncombined_wc_mf = combined_wc_mf %&gt;%\n  mutate(\n    HourOfDay = hour(Time))\n\ncombined_wc_mf = combined_wc_mf %&gt;%\n  mutate(\n    DayOfWeek = factor(\n      wday(ViolationDate, label = TRUE, week_start = 1),\n      levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\nmodel_columns_chrono = c(\n  \"BinaryOutcome\",\n  \"HourOfDay\",\n  \"DayOfMonth\",\n  \"DayOfWeek\",\n  \"District\"\n)\n\ndata_for_modeling = combined_wc_mf %&gt;%\n  select(all_of(model_columns_chrono)) %&gt;%\n  mutate(across(c(HourOfDay, DayOfMonth, DayOfWeek, District), as.factor)) %&gt;%\n  mutate(BinaryOutcome = factor(BinaryOutcome, levels = c(0, 1))) %&gt;%\n  na.omit()\n\ndata_for_modeling\n\n# A tibble: 88,320 × 5\n   BinaryOutcome HourOfDay DayOfMonth DayOfWeek District   \n   &lt;fct&gt;         &lt;fct&gt;     &lt;fct&gt;      &lt;ord&gt;     &lt;fct&gt;      \n 1 0             16        12         Wed       Dranesville\n 2 0             15        13         Mon       Providence \n 3 0             15        13         Mon       Providence \n 4 0             15        13         Mon       Providence \n 5 0             5         9          Thu       Sully      \n 6 0             0         10         Mon       Franconia  \n 7 0             1         10         Mon       Franconia  \n 8 0             13        11         Tue       Dranesville\n 9 0             11        31         Tue       Hunter Mill\n10 0             11        10         Mon       Hunter Mill\n# ℹ 88,310 more rows\n\nset.seed(42)\n\nsample_split_chrono = sample.split(\n  Y = data_for_modeling$BinaryOutcome,\n  SplitRatio = 0.70\n)\n\ntrain_set_chrono = subset(data_for_modeling, sample_split_chrono == TRUE)\ntest_set_chrono = subset(data_for_modeling, sample_split_chrono == FALSE)\n\nlogistic_model_chrono = glm(\n  formula = BinaryOutcome ~ .,\n  data = train_set_chrono,\n  family = binomial(link = \"logit\")\n)\n\nodds_ratio_chrono = exp(coef(logistic_model_chrono))\n\nodds_ratio_df_chrono = data.frame(\n  Variable = names(odds_ratio_chrono),\n  OddsRatio = odds_ratio_chrono\n)\n\nprint(odds_ratio_df_chrono %&gt;%\n        filter(Variable != \"(Intercept)\") %&gt;%\n        arrange(OddsRatio) %&gt;%\n        head(30))\n\n                               Variable OddsRatio\nHourOfDay6                   HourOfDay6 0.3700923\nHourOfDay5                   HourOfDay5 0.5129970\nHourOfDay14                 HourOfDay14 0.6542359\nDayOfMonth25               DayOfMonth25 0.6566403\nHourOfDay13                 HourOfDay13 0.6668524\nHourOfDay11                 HourOfDay11 0.6733436\nHourOfDay16                 HourOfDay16 0.6749441\nDayOfMonth12               DayOfMonth12 0.7050878\nDayOfMonth29               DayOfMonth29 0.7064639\nHourOfDay12                 HourOfDay12 0.7121153\nHourOfDay17                 HourOfDay17 0.7191464\nDayOfMonth11               DayOfMonth11 0.7217906\nDayOfMonth26               DayOfMonth26 0.7264822\nHourOfDay10                 HourOfDay10 0.7278707\nDayOfMonth9                 DayOfMonth9 0.7309660\nDayOfMonth3                 DayOfMonth3 0.7333756\nDayOfMonth4                 DayOfMonth4 0.7421692\nDayOfMonth5                 DayOfMonth5 0.7425681\nHourOfDay15                 HourOfDay15 0.7473944\nDayOfMonth30               DayOfMonth30 0.7492766\nDayOfMonth18               DayOfMonth18 0.7585408\nHourOfDay18                 HourOfDay18 0.7700971\nDayOfMonth20               DayOfMonth20 0.7708255\nDayOfMonth17               DayOfMonth17 0.7771909\nHourOfDay9                   HourOfDay9 0.7811855\nDayOfMonth27               DayOfMonth27 0.7826999\nDayOfMonth21               DayOfMonth21 0.7903996\nDistrictSpringfield DistrictSpringfield 0.7983477\nDayOfMonth8                 DayOfMonth8 0.8061866\nHourOfDay4                   HourOfDay4 0.8063216\n\n\n\n## new visual\ntop_citation_chrono = odds_ratio_df_chrono %&gt;%\n  filter(Variable != \"(Intercept)\") %&gt;%\n  arrange(OddsRatio) %&gt;%\n  head(5) %&gt;%\n  mutate(Effect = \"Stronger Citation Likelihood\")\n\ntop_warning_chrono = odds_ratio_df_chrono %&gt;%\n  filter(Variable != \"(Intercept)\") %&gt;%\n  arrange(desc(OddsRatio)) %&gt;%\n  head(5) %&gt;%\n  mutate(Effect = \"Stronger Warning Likelihood\")\n\nplot_data_chrono = bind_rows(top_citation_chrono, top_warning_chrono)\n\nplot_data_chrono = plot_data_chrono %&gt;%\n  mutate(\n    Variable = gsub(\"HourOfDay\", \"Hr:\", Variable),\n    Variable = gsub(\"DayOfMonth\", \"Day:\", Variable),\n    Variable = gsub(\"DayOfWeek\", \"Week:\", Variable),\n    Variable = gsub(\"District\", \"Dist:\", Variable)\n  )\n\nodds_ratio_plot_chrono = ggplot(plot_data_chrono, aes(x = Variable, y = OddsRatio, fill = Effect)) +\n  geom_bar(stat = \"identity\", position = \"identity\") +\n  geom_hline(yintercept = 1.0, linetype = \"dashed\", color = \"black\", linewidth = 1) +\n  scale_y_continuous(\n    trans = 'log2', \n    breaks = c(0.3, 0.5, 0.7, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.3\", \"0.5\", \"0.7\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(\n    title = \"Odds Ratios of Date and Location Factors on Issuing a Warning\",\n    subtitle = \"Based on Hour, Day of Month, Day of Week, and District\",\n    x = \"Predictor Variable\",\n    y = \"Odds Ratio (Warning vs. Citation)\",\n    fill = \"Enforcement Discretion\"\n  ) +\n  scale_fill_manual(values = c(\"Stronger Citation Likelihood\" = \"coral3\", \"Stronger Warning Likelihood\" = \"gold1\")) +\n  \n  # Flip coordinates for a horizontal bar chart (easier to read labels)\n  coord_flip() +\n  theme_grey(base_size = 11)\n\nodds_ratio_plot_chrono\n\n\n\n\n\n\n\n\n\n## Calculate AUC\nlibrary(pROC)\n\nprobability_chrono = predict(\n  logistic_model_chrono,\n  newdata = test_set_chrono,\n  type = \"response\"\n)\n\nroc_obj_chrono = roc(\n  response = test_set_chrono$BinaryOutcome,\n  predictor = probability_chrono,\n  levels = c(0,1) \n)\n\nSetting direction: controls &lt; cases\n\nauc_value_chrono = auc(roc_obj_chrono)\n\nround(auc_value_chrono, 4)\n\n[1] 0.6289"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Nathania Stephens. I go by Nat. I am a graduate student at George Mason University. I am pursuing a MS in Data Analytics Engineering. Quite fitting, as I have always been passionate about learning and expanding my education in data discovery, literacy and coding. It is such an exciting time to learn with the growth of data and new technology such as Artificial Intelligence. In my career I have been a casual coder, finding opportunities to test out my skills when able. I hope to bring my enhanced knowledge and expanded coding skills back to the workplace to teach others and improve upon my organization’s objectives."
  },
  {
    "objectID": "Helpfulinfo.html",
    "href": "Helpfulinfo.html",
    "title": "Helpful R Info",
    "section": "",
    "text": "Is it a logical (Boolean), string, etc?\n\nx = 5\nclass(x)\n\n[1] \"numeric\"\n\n\nOr use is….?\n\nis.numeric(x)\n\n[1] TRUE\n\n\n\n\n\n\nx = as.character(x)\nx\n\n[1] \"5\"\n\n\n\nis.numeric(x)\n\n[1] FALSE\n\n\n\nclass(x)\n\n[1] \"character\""
  },
  {
    "objectID": "Helpfulinfo.html#identify-data-types",
    "href": "Helpfulinfo.html#identify-data-types",
    "title": "Helpful R Info",
    "section": "",
    "text": "Is it a logical (Boolean), string, etc?\n\nx = 5\nclass(x)\n\n[1] \"numeric\"\n\n\nOr use is….?\n\nis.numeric(x)\n\n[1] TRUE"
  },
  {
    "objectID": "Helpfulinfo.html#change-a-data-type",
    "href": "Helpfulinfo.html#change-a-data-type",
    "title": "Helpful R Info",
    "section": "",
    "text": "x = as.character(x)\nx\n\n[1] \"5\"\n\n\n\nis.numeric(x)\n\n[1] FALSE\n\n\n\nclass(x)\n\n[1] \"character\""
  },
  {
    "objectID": "Helpfulinfo.html#remove-a-variable",
    "href": "Helpfulinfo.html#remove-a-variable",
    "title": "Helpful R Info",
    "section": "Remove a Variable",
    "text": "Remove a Variable\n\nrm(b)\n\n#Vectors Collection of elements, like a list.\n\nmyvector = c('apple', 'banana', 'orange', 'pineapple')\nmyvector\n\n[1] \"apple\"     \"banana\"    \"orange\"    \"pineapple\""
  },
  {
    "objectID": "Helpfulinfo.html#get-current-directory",
    "href": "Helpfulinfo.html#get-current-directory",
    "title": "Helpful R Info",
    "section": "Get current directory",
    "text": "Get current directory\nGet the current working directory you are in.\n\ngetwd()"
  },
  {
    "objectID": "Helpfulinfo.html#change-directory",
    "href": "Helpfulinfo.html#change-directory",
    "title": "Helpful R Info",
    "section": "Change directory",
    "text": "Change directory\nChange your directory another location\n\nsetwd(\"EnTeR NeW PaTh\") # use // or \\ for windows OS"
  },
  {
    "objectID": "Helpfulinfo.html#view",
    "href": "Helpfulinfo.html#view",
    "title": "Helpful R Info",
    "section": "View",
    "text": "View\nTo see a data frame use “View”\n\nView(penguins)"
  },
  {
    "objectID": "Helpfulinfo.html#head",
    "href": "Helpfulinfo.html#head",
    "title": "Helpful R Info",
    "section": "Head",
    "text": "Head\nSee the first few rows of the data frame.\n\nhead(penguins)\n\n  species    island bill_len bill_dep flipper_len body_mass    sex year\n1  Adelie Torgersen     39.1     18.7         181      3750   male 2007\n2  Adelie Torgersen     39.5     17.4         186      3800 female 2007\n3  Adelie Torgersen     40.3     18.0         195      3250 female 2007\n4  Adelie Torgersen       NA       NA          NA        NA   &lt;NA&gt; 2007\n5  Adelie Torgersen     36.7     19.3         193      3450 female 2007\n6  Adelie Torgersen     39.3     20.6         190      3650   male 2007"
  },
  {
    "objectID": "Helpfulinfo.html#structure",
    "href": "Helpfulinfo.html#structure",
    "title": "Helpful R Info",
    "section": "Structure",
    "text": "Structure\n\nstr(penguins)\n\n'data.frame':   344 obs. of  8 variables:\n $ species    : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island     : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_len   : num  39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_dep   : num  18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_len: int  181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass  : int  3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex        : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year       : int  2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ..."
  },
  {
    "objectID": "Helpfulinfo.html#summary",
    "href": "Helpfulinfo.html#summary",
    "title": "Helpful R Info",
    "section": "Summary",
    "text": "Summary\nSummary will provide a summary of the rows, including NA rows.\n\nsummary(penguins)\n\n      species          island       bill_len        bill_dep    \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n  flipper_len      body_mass        sex           year     \n Min.   :172.0   Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0   1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0   Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9   Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0   3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0   Max.   :6300                Max.   :2009  \n NA's   :2       NA's   :2"
  },
  {
    "objectID": "Helpfulinfo.html#glimpse",
    "href": "Helpfulinfo.html#glimpse",
    "title": "Helpful R Info",
    "section": "Glimpse",
    "text": "Glimpse\nUse “glimpse” to see the number of rows, columns, and each of the column names with their data type.\n\nglimpse(penguins)"
  },
  {
    "objectID": "Helpfulinfo.html#check-for-nas",
    "href": "Helpfulinfo.html#check-for-nas",
    "title": "Helpful R Info",
    "section": "Check for NAs",
    "text": "Check for NAs\nCheck for NAs or not available/ missing values. Then determine what you will do with them. Will you get rid of them or insert a value? This only tells you yes and no there are NA values and how much but it wont tell you where those NAs are. However, summary above shows what columns they are in.x\n\ntable(is.na(penguins))\n\n\nFALSE  TRUE \n 2733    19"
  },
  {
    "objectID": "RedesignCode.html",
    "href": "RedesignCode.html",
    "title": "Redesign Code",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(leaflet)\nlibrary(tidyverse)\nlibrary(usmap)\nlibrary(sf)\n\nLinking to GEOS 3.13.1, GDAL 3.11.0, PROJ 9.6.0; sf_use_s2() is TRUE"
  },
  {
    "objectID": "RedesignCode.html#libraries",
    "href": "RedesignCode.html#libraries",
    "title": "Redesign Code",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(leaflet)\nlibrary(tidyverse)\nlibrary(usmap)\nlibrary(sf)\n\nLinking to GEOS 3.13.1, GDAL 3.11.0, PROJ 9.6.0; sf_use_s2() is TRUE"
  },
  {
    "objectID": "RedesignCode.html#default-class-theme",
    "href": "RedesignCode.html#default-class-theme",
    "title": "Redesign Code",
    "section": "Default Class Theme",
    "text": "Default Class Theme\n\nhw &lt;- theme_gray()+ theme(\n  plot.title=element_text(hjust=0.5),\n  plot.subtitle=element_text(hjust=0.5),\n  plot.caption=element_text(hjust=-.5),\n  \n  strip.text.y = element_blank(),\n  strip.background=element_rect(fill=rgb(.9,.95,1),\n                                colour=gray(.5), linewidth =.2),\n  \n  panel.border=element_rect(fill=FALSE,colour=gray(.70)),\n  panel.grid.minor.y = element_blank(),\n  panel.grid.minor.x = element_blank(),\n  panel.spacing.x = unit(0.10,\"cm\"),\n  panel.spacing.y = unit(0.05,\"cm\"),\n  \n  # axis.ticks.y= element_blank()\n  axis.ticks=element_blank(),\n  axis.text=element_text(colour=\"black\"),\n  axis.text.y=element_text(margin=margin(0,3,0,3)),\n  axis.text.x=element_text(margin=margin(-1,0,3,0))\n)"
  },
  {
    "objectID": "RedesignCode.html#redesign-1-bar-chart",
    "href": "RedesignCode.html#redesign-1-bar-chart",
    "title": "Redesign Code",
    "section": "Redesign 1: Bar chart",
    "text": "Redesign 1: Bar chart\n\nnps_sar = read.csv('NPmostSARincidents.csv')\n\nggplot(nps_sar, aes(x = reorder(NationalPark, NumberSARIncidents), y = NumberSARIncidents)) +\n  geom_bar(stat = 'identity', fill = 'forestgreen')  +\n  coord_flip() +\n  labs(\n    title = \"Number of Search and Rescue Incidents by Park\",\n    x = NULL,\n    y = \"Number of Incidents\") + hw"
  },
  {
    "objectID": "RedesignCode.html#redesign-1-chorophelt-map",
    "href": "RedesignCode.html#redesign-1-chorophelt-map",
    "title": "Redesign Code",
    "section": "Redesign 1: Chorophelt map",
    "text": "Redesign 1: Chorophelt map\n\nusa_tbl = map_data(\"state\") %&gt;% as_tibble()\n\n# To ensure the NPS SAR data matches, change state column to lowercase.\nnps_sar$State1 = tolower(nps_sar$State1)\n\n# Group by state and add totals\nlibrary(dplyr)\nsarbystate = nps_sar %&gt;%\n  group_by(State1) %&gt;%\n  summarize(\n    TotalbyState = sum(NumberSARIncidents, na.rm = TRUE)) %&gt;%\n  arrange(desc(TotalbyState))\n\n# Join the state map with the sar table\nusamapsar = left_join(usa_tbl, sarbystate, by = c('region' = 'State1'))\n\nmap1 = ggplot(usamapsar, aes( x = long, y =lat, group= group)) +\n  geom_polygon(aes(fill = TotalbyState), color = 'black') + \n  scale_fill_gradient(name = '# of Incidents', low = 'yellow',\n                      high = 'red', na.value = 'grey80') + \n  coord_map() +\n  theme_minimal() +\n  labs(\n    title = \"Top 11 States with Search and Rescue Incidents\", \n    x ='', \n    y = '',\n    caption = '*Gray states contain no data') + \n  theme(plot.title = element_text(size = 20))\n\nmap1"
  },
  {
    "objectID": "RedesignCode.html#redesign-1-interactive-map",
    "href": "RedesignCode.html#redesign-1-interactive-map",
    "title": "Redesign Code",
    "section": "Redesign 1: Interactive Map",
    "text": "Redesign 1: Interactive Map\n\nnpspoints = read.csv('NPSBoundary.csv')\n\ntopnps = nps_sar$NationalPark\n\ntopnps = c(topnps, 'Sequoia National Park', 'Kings Canyon National Park')\n\nfiltered_npspoints = npspoints %&gt;%\n  filter(UNIT_NAME %in% topnps)\n\nfiltered_npspoints$NumberSARIncidents = c(131,202,165,279,224,341,285,133,371,198,131,785,300,203,732,146,204)\n\nleaflet(filtered_npspoints)%&gt;% \n  addTiles() %&gt;% \n  addMarkers(lng = ~x,\n             lat = ~y,\n             label = ~paste0(UNIT_NAME,\"; Incidents: \", NumberSARIncidents))"
  },
  {
    "objectID": "RedesignCode.html#redesign-2-choropleth-map",
    "href": "RedesignCode.html#redesign-2-choropleth-map",
    "title": "Redesign Code",
    "section": "Redesign 2: Choropleth Map",
    "text": "Redesign 2: Choropleth Map\n\nstatesSAR = read.csv('StatesSAR.csv')\n\n# lower case states\nstatesSAR$State = tolower(statesSAR$State)\n\nstatesSARmap = left_join(usa_tbl, statesSAR, by = c('region' = 'State'))\n\nmapSAR = ggplot(statesSARmap, aes(x = long, y = lat, group = group)) +\n  geom_polygon(aes(fill = NumberSARIncidents), color = 'black') +\n  scale_fill_gradient(name = '# of SAR Incidents', low = 'yellow',\n                      high = 'red', na.value = 'grey80') +\n  coord_map() +\n  labs(\n    title = \"Top 20 States with the Most Search and Rescue Incidents\",\n    x = '',\n    y = '',\n    caption = '*Gray represents no data') +\n  theme(plot.title = element_text(size = 15)) + theme_minimal()\n\nmapSAR"
  },
  {
    "objectID": "RedesignCode.html#redesign-2-choropleth-map-with-alaska",
    "href": "RedesignCode.html#redesign-2-choropleth-map-with-alaska",
    "title": "Redesign Code",
    "section": "Redesign 2: Choropleth map with Alaska",
    "text": "Redesign 2: Choropleth map with Alaska\n\n# Use us_map to get all states but only take full and geom columns\nSARstates = read.csv('StatesSAR.csv')\nusa = us_map('states')\nusa_sub = usa[, c(\"full\", \"geom\")]\nusajoinsar = left_join(usa_sub, SARstates, by = c('full' = 'State'))\n\nggplot() +\n  geom_sf(data=usajoinsar, aes(fill= NumberSARIncidents), color = 'gray70') +\n  scale_fill_gradient(name = '# of SAR Incidents', low = 'yellow', high = 'red', na.value = 'grey30') +\n  labs(title = \"Top 20 States\", \n       subtitle = \"with the Most Search and Rescue Incidents\",\n       caption = c('Gray represents no data')) +\n  theme_void() +\n  theme(title = element_text(face = 'bold'),\n        legend.position = 'bottom')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nat’s Site",
    "section": "",
    "text": "This is a work in progress, but here you will find information about me, my final project which examines Fairfax County crime during 2023, my redesign project which focuses on visualizing US National Parks’ Search and Rescue Incidents, along with the code for each project. Also include on this site are helpful R code and resources I’ve come across since starting my Graduate Program at GMU."
  },
  {
    "objectID": "index.html#welcome-to-my-site.",
    "href": "index.html#welcome-to-my-site.",
    "title": "Nat’s Site",
    "section": "",
    "text": "This is a work in progress, but here you will find information about me, my final project which examines Fairfax County crime during 2023, my redesign project which focuses on visualizing US National Parks’ Search and Rescue Incidents, along with the code for each project. Also include on this site are helpful R code and resources I’ve come across since starting my Graduate Program at GMU."
  },
  {
    "objectID": "FinalProject.html#motivation",
    "href": "FinalProject.html#motivation",
    "title": "Final Project",
    "section": "Motivation",
    "text": "Motivation\nIn 2023, there were over 30,000 arrests and close to 65,000 citations in Fairfax County. The Fairfax County boundaries, include areas such as Centreville, Chantilly, Herndon, Reston, Tysons Corner, McLean, Merrifield, George Mason, Annadale, Burke, Springfield, Alexandria, Lorton to name a few. If you live, work, or study in these areas then this project should be of interest to you. This project aims to inform Fairfax County patrons of violation, warning, and arrest trends and hopefully provide some statistical insights that is applicable."
  },
  {
    "objectID": "FinalProject.html#goals",
    "href": "FinalProject.html#goals",
    "title": "Final Project",
    "section": "Goals",
    "text": "Goals\nProvide relevant, understandable, and insightful crime patterns using several different visualization and statistical methods. Deliver clear and concise graphs and charts that help readers easily interpret and compare data. Utilize statistical learning techniques to examine data, interpret statistical significant factors, and understand associations between variables. Since the data utilized for this project is largely categorical this project focuses on techniques such as Chi-Squared Test and Logistic Regression."
  }
]